#!/bin/bash
#PBS -N adr_scaling_openmpi
#PBS -l select=1:ncpus=28
#PBS -l walltime=24:00:00
#PBS -q cpu
#PBS -A Hybrid-Parallelism-ADR-Solver-PDE2025
#PBS -m abe
#PBS -M peng.rao@mail.polimi.it

#===============================================================================
# Setup
#===============================================================================

cd ${PBS_O_WORKDIR:-$(pwd)}

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
LOG_DIR="logs_scaling"
mkdir -p ${LOG_DIR}
LOG_FILE="${LOG_DIR}/scaling_${PBS_JOBID:-local}_${TIMESTAMP}.log"

RESULTS_DIR="results_scaling_${TIMESTAMP}"
mkdir -p ${RESULTS_DIR}

exec > >(tee -a "${LOG_FILE}") 2>&1

echo "==============================================================================="
echo "SCALING BENCHMARK - OpenMPI Version"
echo "==============================================================================="
echo "Job ID: ${PBS_JOBID}"
echo "Date: $(date)"
echo "Results Dir: ${RESULTS_DIR}"
echo ""
echo "Configuration:"
echo "  - Using system OpenMPI (no spack to avoid library conflicts)"
echo "  - System PETSc (built with OpenMPI)"
echo "  - OpenMPI binding for process affinity"
echo "==============================================================================="

#-------------------------------------------------------------------------------
# Environment Setup - System Libraries Only (No Spack)
#-------------------------------------------------------------------------------
# Clear any conflicting environment variables
unset LD_PRELOAD

# Keep only essential paths, remove spack paths that might conflict
# If you need spack for deal.II, source it but be careful with MPI
if [ -f "/work/u11022931/spack/share/spack/setup-env.sh" ]; then
    source /work/u11022931/spack/share/spack/setup-env.sh
    spack env activate .
    
    # Force system MPI libraries to take precedence
    # This ensures we use system OpenMPI, not spack MPICH
    export LD_LIBRARY_PATH="/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH"
fi

# Verify MPI version
echo ""
echo "MPI Configuration:"
mpirun --version | head -3
echo ""

ulimit -s unlimited

#===============================================================================
# Build
#===============================================================================

BUILD_DIR="build"
EXECUTABLE="${BUILD_DIR}/scaling_study"
REBUILD_REQUIRED=false

if [ ! -f "$EXECUTABLE" ]; then
    REBUILD_REQUIRED=true
else
    NEWEST_SOURCE=$(find . -maxdepth 2 \( -name "*.cpp" -o -name "*.h" -o -name "CMakeLists.txt" \) 2>/dev/null | xargs ls -t 2>/dev/null | head -n 1)
    if [ -n "$NEWEST_SOURCE" ] && [ "$NEWEST_SOURCE" -nt "$EXECUTABLE" ]; then
        REBUILD_REQUIRED=true
    fi
fi

if [ "$REBUILD_REQUIRED" = true ]; then
    echo "Building scaling benchmark..."
    mkdir -p ${BUILD_DIR}
    cd ${BUILD_DIR}
    cmake .. -DCMAKE_BUILD_TYPE=Release
    make -j 28 scaling_study || exit 1
    cd ..
    echo "Build complete."
else
    echo "Binary is up to date."
fi

# Verify library linking
echo ""
echo "Library linking check (MPI libraries):"
ldd ${EXECUTABLE} | grep -E "mpi" | head -5
echo ""

#===============================================================================
# Benchmark Parameters
#===============================================================================

DIM=2
DEGREE=2
WARMUP=1
TRIALS=3

# Problem sizes for 2D Q2 elements:
#   refs=8  -> ~263k DoFs
#   refs=9  -> ~1.05M DoFs
#   refs=10 -> ~4.2M DoFs
#   refs=11 -> ~16.8M DoFs

MIN_REF_STRONG=9
MAX_REF_STRONG=11

MIN_REF_WEAK=7
MAX_REF_WEAK=9

MPI_CMD="mpirun"

#===============================================================================
# OpenMPI Binding Configuration
#===============================================================================
# OpenMPI uses different syntax than MPICH:
#   --bind-to core    : bind each process to a single core
#   --map-by core     : map processes to cores sequentially
#   --bind-to none    : no binding (for hybrid with threads)
#   --report-bindings : print binding info (useful for debugging)

# Pure MPI binding: each MPI process bound to one core
MPI_BIND_CORE="--bind-to core --map-by core"

# Hybrid binding: don't bind to single core (threads need room)
MPI_BIND_NONE="--bind-to none"

# Optional: add --report-bindings to see where processes are placed
# MPI_BIND_CORE="--bind-to core --map-by core --report-bindings"

echo ""
echo "==============================================================================="
echo "OpenMPI Process Binding Configuration"
echo "==============================================================================="
echo "Pure MPI runs:  ${MPI_BIND_CORE}"
echo "Hybrid runs:    ${MPI_BIND_NONE}"
echo "==============================================================================="

echo ""
echo "==============================================================================="
echo "Problem Size Estimates (2D Q2 elements)"
echo "==============================================================================="
echo "refs=8:  ~263,000 DoFs   -> min 5 processes (50k/proc)"
echo "refs=9:  ~1,050,000 DoFs -> min 21 processes (50k/proc)"
echo "refs=10: ~4,200,000 DoFs -> min 84 processes (50k/proc)"
echo "refs=11: ~16,800,000 DoFs -> OK for any config on 28 cores"
echo ""
echo "Selected range: refs ${MIN_REF_STRONG}-${MAX_REF_STRONG}"
echo "==============================================================================="

#===============================================================================
# PHASE 1: PURE MPI STRONG SCALING (Primary Test)
#===============================================================================

echo ""
echo "==============================================================================="
echo "PHASE 1: PURE MPI STRONG SCALING"
echo "==============================================================================="
echo "Testing with 1 thread per MPI process (cleanest scaling curves)"
echo "Using: ${MPI_BIND_CORE}"
echo ""

declare -a PURE_MPI_CONFIGS=(
    "1 1"
    "2 1"
    "4 1"
    "7 1"
    "14 1"
    "28 1"
)

for CONFIG in "${PURE_MPI_CONFIGS[@]}"; do
    read -r NRANKS NTHREADS <<< "$CONFIG"
    TOTAL_CORES=$((NRANKS * NTHREADS))
    CONFIG_NAME="pure_mpi_${NRANKS}"

    echo ""
    echo "-----------------------------------------------------------------------"
    echo "Pure MPI: ${NRANKS} processes × ${NTHREADS} thread = ${TOTAL_CORES} cores"
    echo "Binding: ${MPI_BIND_CORE}"
    echo "-----------------------------------------------------------------------"

    export TBB_NUM_THREADS=${NTHREADS}
    export OMP_NUM_THREADS=${NTHREADS}
    export OMP_PROC_BIND=close
    export OMP_PLACES=cores

    OUTPUT_PREFIX="${RESULTS_DIR}/strong_${CONFIG_NAME}"

    ${MPI_CMD} -np ${NRANKS} ${MPI_BIND_CORE} \
        ${EXECUTABLE} \
        --strong \
        --min-ref ${MIN_REF_STRONG} \
        --max-ref ${MAX_REF_STRONG} \
        --degree ${DEGREE} \
        --dim ${DIM} \
        --output "${OUTPUT_PREFIX}" \
        --threads ${NTHREADS} \
        --warmup ${WARMUP} \
        --trials ${TRIALS} \
        2>&1 | tee "${OUTPUT_PREFIX}.log"

    sleep 2
done

#===============================================================================
# PHASE 2: PURE THREADING SCALING (Secondary Test)
#===============================================================================

echo ""
echo "==============================================================================="
echo "PHASE 2: PURE THREADING SCALING"
echo "==============================================================================="
echo "Testing with 1 MPI process, varying threads"
echo "Using: ${MPI_BIND_NONE} (threads need multiple cores)"
echo ""

declare -a PURE_THREAD_CONFIGS=(
    "1 1"
    "1 2"
    "1 4"
    "1 7"
    "1 14"
    "1 28"
)

for CONFIG in "${PURE_THREAD_CONFIGS[@]}"; do
    read -r NRANKS NTHREADS <<< "$CONFIG"
    TOTAL_CORES=$((NRANKS * NTHREADS))
    CONFIG_NAME="pure_thread_${NTHREADS}"

    # Skip 1x1 (already done in pure MPI)
    if [ "${NRANKS}" -eq 1 ] && [ "${NTHREADS}" -eq 1 ]; then
        continue
    fi

    echo ""
    echo "-----------------------------------------------------------------------"
    echo "Pure Threading: ${NRANKS} process × ${NTHREADS} threads = ${TOTAL_CORES} cores"
    echo "Binding: ${MPI_BIND_NONE}"
    echo "-----------------------------------------------------------------------"

    export TBB_NUM_THREADS=${NTHREADS}
    export OMP_NUM_THREADS=${NTHREADS}
    export OMP_PROC_BIND=spread
    export OMP_PLACES=cores

    OUTPUT_PREFIX="${RESULTS_DIR}/strong_${CONFIG_NAME}"

    ${MPI_CMD} -np ${NRANKS} ${MPI_BIND_NONE} \
        ${EXECUTABLE} \
        --strong \
        --min-ref ${MIN_REF_STRONG} \
        --max-ref ${MAX_REF_STRONG} \
        --degree ${DEGREE} \
        --dim ${DIM} \
        --output "${OUTPUT_PREFIX}" \
        --threads ${NTHREADS} \
        --warmup ${WARMUP} \
        --trials ${TRIALS} \
        2>&1 | tee "${OUTPUT_PREFIX}.log"

    sleep 2
done

#===============================================================================
# PHASE 3: HYBRID CONFIGURATIONS (Comparison at Fixed Total Cores)
#===============================================================================

echo ""
echo "==============================================================================="
echo "PHASE 3: HYBRID ANALYSIS AT FIXED CORE COUNTS"
echo "==============================================================================="
echo "Comparing different MPI×Thread configurations at 14 and 28 total cores"
echo ""

# 14 total cores
declare -a HYBRID_14=(
    "14 1"
    "7 2"
    "2 7"
    "1 14"
)

# 28 total cores
declare -a HYBRID_28=(
    "28 1"
    "14 2"
    "7 4"
    "4 7"
    "2 14"
    "1 28"
)

echo "--- Testing 14 cores ---"
for CONFIG in "${HYBRID_14[@]}"; do
    read -r NRANKS NTHREADS <<< "$CONFIG"
    CONFIG_NAME="hybrid_${NRANKS}x${NTHREADS}"

    echo "  ${NRANKS} MPI × ${NTHREADS} threads"

    export TBB_NUM_THREADS=${NTHREADS}
    export OMP_NUM_THREADS=${NTHREADS}
    export OMP_PROC_BIND=close
    export OMP_PLACES=cores

    OUTPUT_PREFIX="${RESULTS_DIR}/hybrid14_${CONFIG_NAME}"

    # Use bind-to-core only for pure MPI (1 thread), otherwise no binding
    if [ "${NTHREADS}" -eq 1 ]; then
        BIND_OPT="${MPI_BIND_CORE}"
    else
        BIND_OPT="${MPI_BIND_NONE}"
    fi

    ${MPI_CMD} -np ${NRANKS} ${BIND_OPT} \
        ${EXECUTABLE} \
        --strong \
        --min-ref ${MAX_REF_STRONG} \
        --max-ref ${MAX_REF_STRONG} \
        --degree ${DEGREE} \
        --dim ${DIM} \
        --output "${OUTPUT_PREFIX}" \
        --threads ${NTHREADS} \
        --warmup ${WARMUP} \
        --trials ${TRIALS} \
        2>&1 > "${OUTPUT_PREFIX}.log"

    sleep 1
done

echo ""
echo "--- Testing 28 cores ---"
for CONFIG in "${HYBRID_28[@]}"; do
    read -r NRANKS NTHREADS <<< "$CONFIG"
    CONFIG_NAME="hybrid_${NRANKS}x${NTHREADS}"

    echo "  ${NRANKS} MPI × ${NTHREADS} threads"

    export TBB_NUM_THREADS=${NTHREADS}
    export OMP_NUM_THREADS=${NTHREADS}
    export OMP_PROC_BIND=close
    export OMP_PLACES=cores

    OUTPUT_PREFIX="${RESULTS_DIR}/hybrid28_${CONFIG_NAME}"

    if [ "${NTHREADS}" -eq 1 ]; then
        BIND_OPT="${MPI_BIND_CORE}"
    else
        BIND_OPT="${MPI_BIND_NONE}"
    fi

    ${MPI_CMD} -np ${NRANKS} ${BIND_OPT} \
        ${EXECUTABLE} \
        --strong \
        --min-ref ${MAX_REF_STRONG} \
        --max-ref ${MAX_REF_STRONG} \
        --degree ${DEGREE} \
        --dim ${DIM} \
        --output "${OUTPUT_PREFIX}" \
        --threads ${NTHREADS} \
        --warmup ${WARMUP} \
        --trials ${TRIALS} \
        2>&1 > "${OUTPUT_PREFIX}.log"

    sleep 1
done

#===============================================================================
# PHASE 4: WEAK SCALING
#===============================================================================

echo ""
echo "==============================================================================="
echo "PHASE 4: WEAK SCALING (Pure MPI)"
echo "==============================================================================="
echo "Problem size scales with core count to maintain constant work per core"
echo "Using: ${MPI_BIND_CORE}"
echo ""

for CONFIG in "${PURE_MPI_CONFIGS[@]}"; do
    read -r NRANKS NTHREADS <<< "$CONFIG"
    TOTAL_CORES=$((NRANKS * NTHREADS))
    CONFIG_NAME="weak_mpi_${NRANKS}"

    echo ""
    echo "-----------------------------------------------------------------------"
    echo "Weak Scaling: ${NRANKS} processes"
    echo "Binding: ${MPI_BIND_CORE}"
    echo "-----------------------------------------------------------------------"

    export TBB_NUM_THREADS=${NTHREADS}
    export OMP_NUM_THREADS=${NTHREADS}
    export OMP_PROC_BIND=close
    export OMP_PLACES=cores

    OUTPUT_PREFIX="${RESULTS_DIR}/${CONFIG_NAME}"

    ${MPI_CMD} -np ${NRANKS} ${MPI_BIND_CORE} \
        ${EXECUTABLE} \
        --weak \
        --min-ref ${MIN_REF_WEAK} \
        --max-ref ${MAX_REF_WEAK} \
        --degree ${DEGREE} \
        --dim ${DIM} \
        --output "${OUTPUT_PREFIX}" \
        --threads ${NTHREADS} \
        --warmup ${WARMUP} \
        --trials ${TRIALS} \
        2>&1 | tee "${OUTPUT_PREFIX}.log"

    sleep 2
done

#===============================================================================
# Combine Results
#===============================================================================

echo ""
echo "==============================================================================="
echo "Combining Results"
echo "==============================================================================="

COMBINED_PURE_MPI="${RESULTS_DIR}/combined_pure_mpi.csv"
COMBINED_PURE_THREAD="${RESULTS_DIR}/combined_pure_thread.csv"
COMBINED_HYBRID="${RESULTS_DIR}/combined_hybrid.csv"
COMBINED_WEAK="${RESULTS_DIR}/combined_weak.csv"
COMBINED_ALL="${RESULTS_DIR}/combined_all_results.csv"

combine_csv() {
    local output=$1
    shift
    local pattern=$1
    
    first_file=true
    for f in ${RESULTS_DIR}/${pattern}*.csv; do
        if [ -f "$f" ]; then
            if [ "$first_file" = true ]; then
                cat "$f" > ${output}
                first_file=false
            else
                tail -n +2 "$f" >> ${output}
            fi
        fi
    done
    
    if [ -f "${output}" ]; then
        echo "Created: ${output}"
    fi
}

combine_csv "${COMBINED_PURE_MPI}" "strong_pure_mpi_"
combine_csv "${COMBINED_PURE_THREAD}" "strong_pure_thread_"
combine_csv "${COMBINED_HYBRID}" "hybrid"
combine_csv "${COMBINED_WEAK}" "weak_"

# Combine all
first_file=true
for f in "${COMBINED_PURE_MPI}" "${COMBINED_PURE_THREAD}" "${COMBINED_HYBRID}" "${COMBINED_WEAK}"; do
    if [ -f "$f" ]; then
        if [ "$first_file" = true ]; then
            cat "$f" > ${COMBINED_ALL}
            first_file=false
        else
            tail -n +2 "$f" >> ${COMBINED_ALL}
        fi
    fi
done
echo "Created: ${COMBINED_ALL}"

#===============================================================================
# Generate Analysis Script
#===============================================================================

ANALYSIS_SCRIPT="${RESULTS_DIR}/analyze_scaling.py"

cat > ${ANALYSIS_SCRIPT} << 'PYTHON_EOF'
#!/usr/bin/env python3
"""
Scaling Analysis Script
- Uses SOLVE TIME for scaling metrics
- Generates publication-quality plots
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import sys
import os

plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.dpi'] = 150
plt.rcParams['font.size'] = 10

def load_data(filename):
    return pd.read_csv(filename)

def plot_strong_scaling_pure_mpi(df, output_dir):
    """Plot strong scaling using pure MPI data only."""
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    strong_df = df[(df['test_type'] == 'strong_scaling') & (df['n_threads'] == 1)].copy()
    
    if strong_df.empty:
        print("No pure MPI strong scaling data")
        return
    
    colors = {'matrix_based': 'tab:blue', 'matrix_free': 'tab:orange'}
    markers = {'matrix_based': 'o', 'matrix_free': 's'}
    
    for solver in ['matrix_based', 'matrix_free']:
        solver_df = strong_df[strong_df['solver_type'] == solver]
        if solver_df.empty:
            continue
            
        refs_list = sorted(solver_df['n_refinements'].unique())
        
        for refs in refs_list:
            ref_df = solver_df[solver_df['n_refinements'] == refs].sort_values('n_mpi')
            if len(ref_df) < 2:
                continue
            
            label = f"{solver.replace('_', '-')} (refs={refs})"
            color = colors[solver]
            marker = markers[solver]
            alpha = 0.4 + 0.2 * (refs - min(refs_list))
            
            cores = ref_df['n_mpi'].values
            
            baseline_solve = ref_df['solve_time_avg'].iloc[0]
            speedup = baseline_solve / ref_df['solve_time_avg']
            efficiency = speedup / (cores / cores[0])
            throughput = ref_df['n_dofs'].iloc[0] / ref_df['solve_time_avg']
            
            axes[0, 0].plot(cores, ref_df['solve_time_avg'], f'{marker}-', 
                          label=label, color=color, alpha=alpha, markersize=6)
            axes[0, 1].plot(cores, speedup, f'{marker}-', 
                          label=label, color=color, alpha=alpha, markersize=6)
            axes[1, 0].plot(cores, efficiency, f'{marker}-', 
                          label=label, color=color, alpha=alpha, markersize=6)
            axes[1, 1].plot(cores, throughput, f'{marker}-', 
                          label=label, color=color, alpha=alpha, markersize=6)
    
    max_cores = strong_df['n_mpi'].max()
    axes[0, 1].plot([1, max_cores], [1, max_cores], 'k--', lw=2, label='Ideal')
    axes[1, 0].axhline(y=1.0, color='k', linestyle='--', lw=2, label='Ideal (100%)')
    
    axes[0, 0].set_xlabel('MPI Processes')
    axes[0, 0].set_ylabel('Solve Time (s)')
    axes[0, 0].set_title('Strong Scaling: Solve Time\n(Pure MPI, 1 thread/process)')
    axes[0, 0].legend(fontsize=8)
    axes[0, 0].set_xscale('log', base=2)
    axes[0, 0].set_yscale('log')
    
    axes[0, 1].set_xlabel('MPI Processes')
    axes[0, 1].set_ylabel('Speedup')
    axes[0, 1].set_title('Strong Scaling: Speedup')
    axes[0, 1].legend(fontsize=8)
    axes[0, 1].set_xscale('log', base=2)
    axes[0, 1].set_yscale('log', base=2)
    
    axes[1, 0].set_xlabel('MPI Processes')
    axes[1, 0].set_ylabel('Parallel Efficiency')
    axes[1, 0].set_title('Strong Scaling: Efficiency')
    axes[1, 0].legend(fontsize=8)
    axes[1, 0].set_xscale('log', base=2)
    axes[1, 0].set_ylim([0, 1.2])
    
    axes[1, 1].set_xlabel('MPI Processes')
    axes[1, 1].set_ylabel('DoFs/second')
    axes[1, 1].set_title('Strong Scaling: Throughput')
    axes[1, 1].legend(fontsize=8)
    axes[1, 1].set_xscale('log', base=2)
    axes[1, 1].set_yscale('log')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'strong_scaling_pure_mpi.png'))
    plt.close()
    print("Saved: strong_scaling_pure_mpi.png")

def plot_weak_scaling(df, output_dir):
    """Plot weak scaling results."""
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    weak_df = df[df['test_type'] == 'weak_scaling'].copy()
    if weak_df.empty:
        print("No weak scaling data")
        return
    
    weak_df = weak_df[weak_df['n_threads'] == 1]
    
    for solver in ['matrix_based', 'matrix_free']:
        solver_df = weak_df[weak_df['solver_type'] == solver].sort_values('n_mpi')
        if solver_df.empty:
            continue
        
        color = 'tab:blue' if solver == 'matrix_based' else 'tab:orange'
        marker = 'o' if solver == 'matrix_based' else 's'
        label = solver.replace('_', '-')
        
        axes[0].plot(solver_df['n_mpi'], solver_df['solve_time_avg'], 
                    f'{marker}-', label=label, color=color, markersize=8)
        
        if len(solver_df) > 0:
            baseline = solver_df['solve_time_avg'].iloc[0]
            efficiency = baseline / solver_df['solve_time_avg']
            axes[1].plot(solver_df['n_mpi'], efficiency,
                        f'{marker}-', label=label, color=color, markersize=8)
    
    axes[0].set_xlabel('MPI Processes')
    axes[0].set_ylabel('Solve Time (s)')
    axes[0].set_title('Weak Scaling: Solve Time\n(should be constant for ideal scaling)')
    axes[0].legend()
    axes[0].set_xscale('log', base=2)
    
    axes[1].axhline(y=1.0, color='k', linestyle='--', lw=2, label='Ideal')
    axes[1].set_xlabel('MPI Processes')
    axes[1].set_ylabel('Efficiency = T(1)/T(p)')
    axes[1].set_title('Weak Scaling: Efficiency')
    axes[1].legend()
    axes[1].set_xscale('log', base=2)
    axes[1].set_ylim([0, 1.5])
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'weak_scaling.png'))
    plt.close()
    print("Saved: weak_scaling.png")

def print_summary(df):
    """Print data summary."""
    print("\n" + "="*70)
    print("DATA SUMMARY")
    print("="*70)
    print(f"Total records: {len(df)}")
    print(f"Solvers: {df['solver_type'].unique().tolist()}")
    print(f"Test types: {df['test_type'].unique().tolist()}")
    
    print("\nStrong scaling (pure MPI) summary:")
    strong_mpi = df[(df['test_type'] == 'strong_scaling') & (df['n_threads'] == 1)]
    for solver in strong_mpi['solver_type'].unique():
        solver_df = strong_mpi[strong_mpi['solver_type'] == solver]
        for refs in sorted(solver_df['n_refinements'].unique()):
            ref_df = solver_df[solver_df['n_refinements'] == refs].sort_values('n_mpi')
            if len(ref_df) >= 2:
                t1 = ref_df['solve_time_avg'].iloc[0]
                tn = ref_df['solve_time_avg'].iloc[-1]
                n1 = ref_df['n_mpi'].iloc[0]
                nn = ref_df['n_mpi'].iloc[-1]
                speedup = t1 / tn
                ideal = nn / n1
                eff = speedup / ideal * 100
                print(f"  {solver} refs={refs}: T({n1})={t1:.3f}s -> T({nn})={tn:.3f}s, "
                      f"Speedup={speedup:.1f}x (ideal={ideal:.0f}x), Eff={eff:.0f}%")

def main():
    if len(sys.argv) < 2:
        print("Usage: python analyze_scaling.py <results_csv>")
        sys.exit(1)
    
    csv_file = sys.argv[1]
    output_dir = os.path.dirname(csv_file) or '.'
    
    print(f"Loading: {csv_file}")
    df = load_data(csv_file)
    
    print_summary(df)
    
    print("\nGenerating plots...")
    plot_strong_scaling_pure_mpi(df, output_dir)
    plot_weak_scaling(df, output_dir)
    
    print("\nDone!")

if __name__ == "__main__":
    main()
PYTHON_EOF

chmod +x ${ANALYSIS_SCRIPT}

#===============================================================================
# Final Summary
#===============================================================================

echo ""
echo "==============================================================================="
echo "BENCHMARK COMPLETE"
echo "==============================================================================="
echo "Results: ${RESULTS_DIR}"
echo ""
echo "Key output files:"
echo "  ${COMBINED_PURE_MPI}     - Pure MPI scaling (primary)"
echo "  ${COMBINED_PURE_THREAD}  - Pure threading scaling"
echo "  ${COMBINED_HYBRID}       - Hybrid configurations"
echo "  ${COMBINED_WEAK}         - Weak scaling"
echo "  ${COMBINED_ALL}          - All results combined"
echo ""
echo "To generate plots:"
echo "  python ${ANALYSIS_SCRIPT} ${COMBINED_ALL}"
echo ""
echo "OpenMPI binding used:"
echo "  Pure MPI:  ${MPI_BIND_CORE}"
echo "  Hybrid:    ${MPI_BIND_NONE}"
echo ""
echo "End: $(date)"
echo "==============================================================================="
