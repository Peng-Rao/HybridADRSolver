#!/bin/bash

#===============================================================================
# PBS Job Script for Strong and Weak Scaling Benchmark
#===============================================================================
# Tests both matrix-based and matrix-free solvers across different
# parallelization configurations to measure scaling efficiency.
#
# Strong Scaling: Fixed problem size, varying core count
# Weak Scaling: Problem size scales with core count
#===============================================================================

#PBS -N adr_scaling_benchmark
#PBS -l select=1:ncpus=28
#PBS -l walltime=24:00:00
#PBS -q cpu
#PBS -A Hybrid-Parallelism-ADR-Solver-PDE2025
#PBS -m abe
#PBS -M peng.rao@mail.polimi.it

#===============================================================================
# Setup
#===============================================================================

cd ${PBS_O_WORKDIR:-$(pwd)}

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
LOG_DIR="logs_scaling"
mkdir -p ${LOG_DIR}
LOG_FILE="${LOG_DIR}/scaling_${PBS_JOBID:-local}_${TIMESTAMP}.log"

RESULTS_DIR="results_scaling_${TIMESTAMP}"
mkdir -p ${RESULTS_DIR}

exec > >(tee -a "${LOG_FILE}") 2>&1

echo "==============================================================================="
echo "STRONG & WEAK SCALING BENCHMARK"
echo "==============================================================================="
echo "Job ID: ${PBS_JOBID}"
echo "Date: $(date)"
echo "Results Dir: ${RESULTS_DIR}"
echo "==============================================================================="

#-------------------------------------------------------------------------------
# Spack Environment
#-------------------------------------------------------------------------------
if [ -f "/work/u11022931/spack/share/spack/setup-env.sh" ]; then
    source /work/u11022931/spack/share/spack/setup-env.sh
elif [ -f "/opt/spack/share/spack/setup-env.sh" ]; then
    source /opt/spack/share/spack/setup-env.sh
else
    echo "ERROR: Spack not found"
    exit 1
fi

spack env activate .
echo "Spack Environment: $(spack env status)"
echo ""

ulimit -s unlimited

#===============================================================================
# Build
#===============================================================================

BUILD_DIR="build"
EXECUTABLE="${BUILD_DIR}/scaling_benchmark"
REBUILD_REQUIRED=false

if [ ! -f "$EXECUTABLE" ]; then
    REBUILD_REQUIRED=true
else
    NEWEST_SOURCE=$(find . -maxdepth 2 \( -name "*.cpp" -o -name "*.h" -o -name "CMakeLists.txt" \) 2>/dev/null | xargs ls -t 2>/dev/null | head -n 1)
    if [ -n "$NEWEST_SOURCE" ] && [ "$NEWEST_SOURCE" -nt "$EXECUTABLE" ]; then
        REBUILD_REQUIRED=true
    fi
fi

if [ "$REBUILD_REQUIRED" = true ]; then
    echo "Building scaling benchmark..."
    mkdir -p ${BUILD_DIR}
    cd ${BUILD_DIR}
    cmake .. -DCMAKE_BUILD_TYPE=Release
    make -j 28 scaling_benchmark || exit 1
    cd ..
    echo "Build complete."
else
    echo "Binary is up to date."
fi

#===============================================================================
# Benchmark Parameters
#===============================================================================

# Dimension (2 or 3)
DIM=2

# Refinement range
MIN_REF=3
MAX_REF=8

# Polynomial degree
DEGREE=2

# Timing trials
WARMUP=1
TRIALS=3

MPI_CMD="mpirun"

echo "==============================================================================="
echo "MPI Implementation"
echo "==============================================================================="
${MPI_CMD} --version
echo ""

#===============================================================================
# Parallelism Configurations for Scaling Tests
#===============================================================================
# Format: "mpi_ranks threads_per_rank description"
# Total cores = mpi_ranks × threads_per_rank

# Strong scaling: vary total cores used
declare -a STRONG_CONFIGS=(
    # Single core baseline
    "1 1 Baseline_1core"
    
    # Pure MPI scaling
    "2 1 MPI_2"
    "4 1 MPI_4"
    "7 1 MPI_7"
    "14 1 MPI_14"
    "28 1 MPI_28"
    
    # Hybrid scaling (fixed 2 threads per rank)
    "1 2 Hybrid_1x2"
    "2 2 Hybrid_2x2"
    "4 2 Hybrid_4x2"
    "7 2 Hybrid_7x2"
    "14 2 Hybrid_14x2"
    
    # Hybrid scaling (fixed 4 threads per rank)
    "1 4 Hybrid_1x4"
    "2 4 Hybrid_2x4"
    "4 4 Hybrid_4x4"
    "7 4 Hybrid_7x4"
    
    # Thread-heavy configurations
    "1 7 Hybrid_1x7"
    "2 7 Hybrid_2x7"
    "4 7 Hybrid_4x7"
    
    # Pure threading
    "1 14 Thread_14"
    "1 28 Thread_28"
)

# Weak scaling: constant work per core (varied problem size)
declare -a WEAK_CONFIGS=(
    "1 1 Weak_1core"
    "2 1 Weak_2core"
    "4 1 Weak_4core"
    "8 1 Weak_8core"
    "14 1 Weak_14core"
    "28 1 Weak_28core"
    
    # Hybrid weak scaling
    "1 2 Weak_Hybrid_1x2"
    "2 2 Weak_Hybrid_2x2"
    "4 2 Weak_Hybrid_4x2"
    "7 2 Weak_Hybrid_7x2"
    "14 2 Weak_Hybrid_14x2"
)

echo "==============================================================================="
echo "PHASE 1: STRONG SCALING TESTS"
echo "==============================================================================="
echo "Testing scaling efficiency with fixed problem size"
echo "Refinement level: ${MAX_REF} (for consistent large problem)"
echo ""

# Store baseline times for efficiency calculation
BASELINE_TIME_MB=""
BASELINE_TIME_MF=""

CONFIG_COUNT=0
TOTAL_CONFIGS=${#STRONG_CONFIGS[@]}

for CONFIG in "${STRONG_CONFIGS[@]}"; do
    CONFIG_COUNT=$((CONFIG_COUNT + 1))

    read -r NRANKS NTHREADS CONFIG_NAME <<< "$CONFIG"
    TOTAL_CORES=$((NRANKS * NTHREADS))

    echo ""
    echo "==============================================================================="
    echo "[Strong Scaling] Config ${CONFIG_COUNT}/${TOTAL_CONFIGS}: ${CONFIG_NAME}"
    echo "==============================================================================="
    echo "MPI Ranks:         ${NRANKS}"
    echo "Threads/Rank:      ${NTHREADS}"
    echo "Total Cores Used:  ${TOTAL_CORES}/28"
    echo "==============================================================================="

    # Set threading environment
    export TBB_NUM_THREADS=${NTHREADS}
    export OMP_NUM_THREADS=${NTHREADS}
    export OMP_PROC_BIND=close
    export OMP_PLACES=cores
    export OMP_STACKSIZE=64M

    # Output file
    OUTPUT_CSV="${RESULTS_DIR}/strong_${CONFIG_NAME}.csv"
    OUTPUT_LOG="${RESULTS_DIR}/strong_log_${CONFIG_NAME}.txt"

    echo "Output: ${OUTPUT_CSV}"
    echo ""

    # Run strong scaling benchmark
    ${MPI_CMD} -n ${NRANKS} \
        ${EXECUTABLE} \
        --strong \
        --min-ref ${MIN_REF} \
        --max-ref ${MAX_REF} \
        --degree ${DEGREE} \
        --dim ${DIM} \
        --output "${RESULTS_DIR}/strong_${CONFIG_NAME}" \
        --threads ${NTHREADS} \
        --warmup ${WARMUP} \
        --trials ${TRIALS} \
        2>&1 | tee ${OUTPUT_LOG}

    EXIT_CODE=$?

    if [ $EXIT_CODE -ne 0 ]; then
        echo "WARNING: Configuration ${CONFIG_NAME} failed with exit code ${EXIT_CODE}"
    else
        echo "Configuration ${CONFIG_NAME} completed successfully."
    fi

    sleep 2
done

echo ""
echo "==============================================================================="
echo "PHASE 2: WEAK SCALING TESTS"
echo "==============================================================================="
echo "Testing efficiency with problem size proportional to core count"
echo "Base refinement: ${MIN_REF}"
echo ""

CONFIG_COUNT=0
TOTAL_CONFIGS=${#WEAK_CONFIGS[@]}

for CONFIG in "${WEAK_CONFIGS[@]}"; do
    CONFIG_COUNT=$((CONFIG_COUNT + 1))

    read -r NRANKS NTHREADS CONFIG_NAME <<< "$CONFIG"
    TOTAL_CORES=$((NRANKS * NTHREADS))

    echo ""
    echo "==============================================================================="
    echo "[Weak Scaling] Config ${CONFIG_COUNT}/${TOTAL_CONFIGS}: ${CONFIG_NAME}"
    echo "==============================================================================="
    echo "MPI Ranks:         ${NRANKS}"
    echo "Threads/Rank:      ${NTHREADS}"
    echo "Total Cores Used:  ${TOTAL_CORES}/28"
    echo "==============================================================================="

    # Set threading environment
    export TBB_NUM_THREADS=${NTHREADS}
    export OMP_NUM_THREADS=${NTHREADS}
    export OMP_PROC_BIND=close
    export OMP_PLACES=cores
    export OMP_STACKSIZE=64M

    # Output file
    OUTPUT_CSV="${RESULTS_DIR}/weak_${CONFIG_NAME}.csv"
    OUTPUT_LOG="${RESULTS_DIR}/weak_log_${CONFIG_NAME}.txt"

    echo "Output: ${OUTPUT_CSV}"
    echo ""

    # Run weak scaling benchmark
    ${MPI_CMD} -n ${NRANKS} \
        ${EXECUTABLE} \
        --weak \
        --min-ref ${MIN_REF} \
        --max-ref $((MAX_REF - 2)) \
        --degree ${DEGREE} \
        --dim ${DIM} \
        --output "${RESULTS_DIR}/weak_${CONFIG_NAME}" \
        --threads ${NTHREADS} \
        --warmup ${WARMUP} \
        --trials ${TRIALS} \
        2>&1 | tee ${OUTPUT_LOG}

    EXIT_CODE=$?

    if [ $EXIT_CODE -ne 0 ]; then
        echo "WARNING: Configuration ${CONFIG_NAME} failed with exit code ${EXIT_CODE}"
    else
        echo "Configuration ${CONFIG_NAME} completed successfully."
    fi

    sleep 2
done

#===============================================================================
# Combine All Results
#===============================================================================

echo ""
echo "==============================================================================="
echo "Combining Results"
echo "==============================================================================="

COMBINED_STRONG="${RESULTS_DIR}/combined_strong_scaling.csv"
COMBINED_WEAK="${RESULTS_DIR}/combined_weak_scaling.csv"
COMBINED_ALL="${RESULTS_DIR}/combined_all_results.csv"

# Combine strong scaling results
first_file=true
for f in ${RESULTS_DIR}/strong_*.csv; do
    if [ -f "$f" ]; then
        if [ "$first_file" = true ]; then
            cat "$f" > ${COMBINED_STRONG}
            first_file=false
        else
            tail -n +2 "$f" >> ${COMBINED_STRONG}
        fi
    fi
done

# Combine weak scaling results
first_file=true
for f in ${RESULTS_DIR}/weak_*.csv; do
    if [ -f "$f" ]; then
        if [ "$first_file" = true ]; then
            cat "$f" > ${COMBINED_WEAK}
            first_file=false
        else
            tail -n +2 "$f" >> ${COMBINED_WEAK}
        fi
    fi
done

# Combine all results
if [ -f "${COMBINED_STRONG}" ]; then
    cat ${COMBINED_STRONG} > ${COMBINED_ALL}
    if [ -f "${COMBINED_WEAK}" ]; then
        tail -n +2 ${COMBINED_WEAK} >> ${COMBINED_ALL}
    fi
elif [ -f "${COMBINED_WEAK}" ]; then
    cat ${COMBINED_WEAK} > ${COMBINED_ALL}
fi

echo "Combined strong scaling: ${COMBINED_STRONG}"
echo "Combined weak scaling: ${COMBINED_WEAK}"
echo "Combined all results: ${COMBINED_ALL}"

#===============================================================================
# Generate Summary Report
#===============================================================================

echo ""
echo "==============================================================================="
echo "Generating Summary Report"
echo "==============================================================================="

SUMMARY_FILE="${RESULTS_DIR}/SCALING_SUMMARY.txt"

cat > ${SUMMARY_FILE} << EOF
Scaling Benchmark Summary
=========================
Date: $(date)
Job ID: ${PBS_JOBID}

System Configuration:
- Node: 1
- Total Cores Available: 28
- Dimension: ${DIM}D
- Refinement Range: ${MIN_REF} to ${MAX_REF}
- Polynomial Degree: ${DEGREE}
- Warmup Runs: ${WARMUP}
- Timed Trials: ${TRIALS}

Strong Scaling Configurations (${#STRONG_CONFIGS[@]} total):
------------------------------------------------------------
EOF

for CONFIG in "${STRONG_CONFIGS[@]}"; do
    read -r NRANKS NTHREADS CONFIG_NAME <<< "$CONFIG"
    TOTAL_CORES=$((NRANKS * NTHREADS))
    printf "%-25s MPI: %2d × Threads: %2d = %2d cores\n" \
        "${CONFIG_NAME}" ${NRANKS} ${NTHREADS} ${TOTAL_CORES} >> ${SUMMARY_FILE}
done

cat >> ${SUMMARY_FILE} << EOF

Weak Scaling Configurations (${#WEAK_CONFIGS[@]} total):
---------------------------------------------------------
EOF

for CONFIG in "${WEAK_CONFIGS[@]}"; do
    read -r NRANKS NTHREADS CONFIG_NAME <<< "$CONFIG"
    TOTAL_CORES=$((NRANKS * NTHREADS))
    printf "%-25s MPI: %2d × Threads: %2d = %2d cores\n" \
        "${CONFIG_NAME}" ${NRANKS} ${NTHREADS} ${TOTAL_CORES} >> ${SUMMARY_FILE}
done

cat >> ${SUMMARY_FILE} << EOF

Results Directory: ${RESULTS_DIR}

Files Generated:
----------------
EOF

ls -lh ${RESULTS_DIR}/*.csv >> ${SUMMARY_FILE} 2>/dev/null

cat ${SUMMARY_FILE}

#===============================================================================
# Generate Python Analysis Script
#===============================================================================

ANALYSIS_SCRIPT="${RESULTS_DIR}/analyze_scaling.py"

cat > ${ANALYSIS_SCRIPT} << 'PYTHON_EOF'
#!/usr/bin/env python3
"""
Scaling Analysis Script
Generates plots for strong and weak scaling benchmarks.
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import sys
import os

def load_data(filename):
    """Load CSV data."""
    return pd.read_csv(filename)

def plot_strong_scaling(df, output_dir):
    """Generate strong scaling plots."""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # Filter strong scaling data
    strong_df = df[df['test_type'] == 'strong_scaling']
    
    if strong_df.empty:
        print("No strong scaling data found")
        return
    
    for solver in ['matrix_based', 'matrix_free']:
        solver_df = strong_df[strong_df['solver_type'] == solver]
        if solver_df.empty:
            continue
        
        # Group by refinement level
        for refs in solver_df['n_refinements'].unique():
            ref_df = solver_df[solver_df['n_refinements'] == refs]
            ref_df = ref_df.sort_values('total_cores')
            
            label = f"{solver} (refs={refs})"
            
            # Time vs cores
            axes[0, 0].plot(ref_df['total_cores'], ref_df['total_time_avg'], 
                          'o-', label=label)
            
            # Speedup (relative to smallest core count in this series)
            baseline = ref_df['total_time_avg'].iloc[0]
            speedup = baseline / ref_df['total_time_avg']
            axes[0, 1].plot(ref_df['total_cores'], speedup, 'o-', label=label)
    
    # Ideal speedup line
    max_cores = strong_df['total_cores'].max()
    ideal_cores = np.array([1, max_cores])
    axes[0, 1].plot(ideal_cores, ideal_cores, 'k--', label='Ideal', linewidth=2)
    
    axes[0, 0].set_xlabel('Total Cores')
    axes[0, 0].set_ylabel('Time (s)')
    axes[0, 0].set_title('Strong Scaling: Execution Time')
    axes[0, 0].legend()
    axes[0, 0].set_xscale('log', base=2)
    axes[0, 0].set_yscale('log')
    axes[0, 0].grid(True)
    
    axes[0, 1].set_xlabel('Total Cores')
    axes[0, 1].set_ylabel('Speedup')
    axes[0, 1].set_title('Strong Scaling: Speedup')
    axes[0, 1].legend()
    axes[0, 1].set_xscale('log', base=2)
    axes[0, 1].set_yscale('log', base=2)
    axes[0, 1].grid(True)
    
    # Efficiency plot
    for solver in ['matrix_based', 'matrix_free']:
        solver_df = strong_df[strong_df['solver_type'] == solver]
        if solver_df.empty:
            continue
        for refs in solver_df['n_refinements'].unique():
            ref_df = solver_df[solver_df['n_refinements'] == refs]
            ref_df = ref_df.sort_values('total_cores')
            label = f"{solver} (refs={refs})"
            axes[1, 0].plot(ref_df['total_cores'], ref_df['parallel_efficiency'], 
                          'o-', label=label)
    
    axes[1, 0].axhline(y=1.0, color='k', linestyle='--', linewidth=2, label='Ideal')
    axes[1, 0].set_xlabel('Total Cores')
    axes[1, 0].set_ylabel('Parallel Efficiency')
    axes[1, 0].set_title('Strong Scaling: Efficiency')
    axes[1, 0].legend()
    axes[1, 0].set_xscale('log', base=2)
    axes[1, 0].set_ylim([0, 1.2])
    axes[1, 0].grid(True)
    
    # Throughput (DoFs/s)
    for solver in ['matrix_based', 'matrix_free']:
        solver_df = strong_df[strong_df['solver_type'] == solver]
        if solver_df.empty:
            continue
        for refs in solver_df['n_refinements'].unique():
            ref_df = solver_df[solver_df['n_refinements'] == refs]
            ref_df = ref_df.sort_values('total_cores')
            label = f"{solver} (refs={refs})"
            axes[1, 1].plot(ref_df['total_cores'], ref_df['dofs_per_second'], 
                          'o-', label=label)
    
    axes[1, 1].set_xlabel('Total Cores')
    axes[1, 1].set_ylabel('DoFs/second')
    axes[1, 1].set_title('Strong Scaling: Throughput')
    axes[1, 1].legend()
    axes[1, 1].set_xscale('log', base=2)
    axes[1, 1].set_yscale('log')
    axes[1, 1].grid(True)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'strong_scaling.png'), dpi=150)
    plt.close()
    print(f"Saved: {os.path.join(output_dir, 'strong_scaling.png')}")

def plot_weak_scaling(df, output_dir):
    """Generate weak scaling plots."""
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    weak_df = df[df['test_type'] == 'weak_scaling']
    
    if weak_df.empty:
        print("No weak scaling data found")
        return
    
    for solver in ['matrix_based', 'matrix_free']:
        solver_df = weak_df[weak_df['solver_type'] == solver]
        if solver_df.empty:
            continue
        
        solver_df = solver_df.sort_values('total_cores')
        
        # Time vs cores (should be constant for perfect weak scaling)
        axes[0].plot(solver_df['total_cores'], solver_df['total_time_avg'], 
                    'o-', label=solver)
        
        # Efficiency
        axes[1].plot(solver_df['total_cores'], solver_df['parallel_efficiency'], 
                    'o-', label=solver)
    
    axes[0].set_xlabel('Total Cores')
    axes[0].set_ylabel('Time (s)')
    axes[0].set_title('Weak Scaling: Execution Time')
    axes[0].legend()
    axes[0].set_xscale('log', base=2)
    axes[0].grid(True)
    
    axes[1].axhline(y=1.0, color='k', linestyle='--', linewidth=2, label='Ideal')
    axes[1].set_xlabel('Total Cores')
    axes[1].set_ylabel('Efficiency')
    axes[1].set_title('Weak Scaling: Efficiency')
    axes[1].legend()
    axes[1].set_xscale('log', base=2)
    axes[1].set_ylim([0, 1.2])
    axes[1].grid(True)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'weak_scaling.png'), dpi=150)
    plt.close()
    print(f"Saved: {os.path.join(output_dir, 'weak_scaling.png')}")

def plot_solver_comparison(df, output_dir):
    """Compare matrix-based vs matrix-free solvers."""
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # Get unique core counts
    cores = sorted(df['total_cores'].unique())
    
    mb_times = []
    mf_times = []
    mb_mem = []
    mf_mem = []
    
    for c in cores:
        mb_df = df[(df['total_cores'] == c) & (df['solver_type'] == 'matrix_based')]
        mf_df = df[(df['total_cores'] == c) & (df['solver_type'] == 'matrix_free')]
        
        if not mb_df.empty:
            mb_times.append(mb_df['total_time_avg'].mean())
            mb_mem.append(mb_df['memory_mb'].mean())
        else:
            mb_times.append(np.nan)
            mb_mem.append(np.nan)
        
        if not mf_df.empty:
            mf_times.append(mf_df['total_time_avg'].mean())
            mf_mem.append(mf_df['memory_mb'].mean())
        else:
            mf_times.append(np.nan)
            mf_mem.append(np.nan)
    
    # Time comparison
    x = np.arange(len(cores))
    width = 0.35
    axes[0].bar(x - width/2, mb_times, width, label='Matrix-Based')
    axes[0].bar(x + width/2, mf_times, width, label='Matrix-Free')
    axes[0].set_xlabel('Total Cores')
    axes[0].set_ylabel('Time (s)')
    axes[0].set_title('Execution Time Comparison')
    axes[0].set_xticks(x)
    axes[0].set_xticklabels(cores)
    axes[0].legend()
    axes[0].grid(True, axis='y')
    
    # Speedup (MF vs MB)
    speedup = [mb/mf if mf > 0 else 0 for mb, mf in zip(mb_times, mf_times)]
    axes[1].bar(x, speedup, color='green')
    axes[1].axhline(y=1.0, color='r', linestyle='--', linewidth=2)
    axes[1].set_xlabel('Total Cores')
    axes[1].set_ylabel('Speedup (MB/MF)')
    axes[1].set_title('Matrix-Free Speedup over Matrix-Based')
    axes[1].set_xticks(x)
    axes[1].set_xticklabels(cores)
    axes[1].grid(True, axis='y')
    
    # Memory comparison
    axes[2].bar(x - width/2, mb_mem, width, label='Matrix-Based')
    axes[2].bar(x + width/2, mf_mem, width, label='Matrix-Free')
    axes[2].set_xlabel('Total Cores')
    axes[2].set_ylabel('Memory (MB)')
    axes[2].set_title('Memory Usage Comparison')
    axes[2].set_xticks(x)
    axes[2].set_xticklabels(cores)
    axes[2].legend()
    axes[2].grid(True, axis='y')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'solver_comparison.png'), dpi=150)
    plt.close()
    print(f"Saved: {os.path.join(output_dir, 'solver_comparison.png')}")

def main():
    if len(sys.argv) < 2:
        print("Usage: python analyze_scaling.py <results_csv>")
        sys.exit(1)
    
    csv_file = sys.argv[1]
    output_dir = os.path.dirname(csv_file) or '.'
    
    print(f"Loading data from: {csv_file}")
    df = load_data(csv_file)
    
    print(f"Loaded {len(df)} records")
    print(f"Solver types: {df['solver_type'].unique()}")
    print(f"Test types: {df['test_type'].unique()}")
    
    plot_strong_scaling(df, output_dir)
    plot_weak_scaling(df, output_dir)
    plot_solver_comparison(df, output_dir)
    
    print("\nAnalysis complete!")

if __name__ == "__main__":
    main()
PYTHON_EOF

chmod +x ${ANALYSIS_SCRIPT}
echo "Analysis script generated: ${ANALYSIS_SCRIPT}"

#===============================================================================
# Final Summary
#===============================================================================

echo ""
echo "==============================================================================="
echo "SCALING BENCHMARK COMPLETE"
echo "==============================================================================="
echo "Results Directory: ${RESULTS_DIR}"
echo "Log File: ${LOG_FILE}"
echo "Summary: ${SUMMARY_FILE}"
echo ""
echo "Combined Results:"
echo "  Strong Scaling: ${COMBINED_STRONG}"
echo "  Weak Scaling:   ${COMBINED_WEAK}"
echo "  All Results:    ${COMBINED_ALL}"
echo ""
echo "To generate plots, run:"
echo "  python ${ANALYSIS_SCRIPT} ${COMBINED_ALL}"
echo ""
echo "End Time: $(date)"
echo "==============================================================================="
echo ""
