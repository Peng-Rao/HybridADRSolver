#!/bin/bash

#===============================================================================
# FIXED PBS Job Script for Scaling Benchmark
#===============================================================================
# Key fixes:
# 1. Uses larger problem sizes (refs 9-11 for 2D) to ensure meaningful scaling
# 2. Simplified configurations - no redundant MPI×thread combos at same total
# 3. Pure MPI scaling as primary test (cleanest results)
# 4. Separate hybrid analysis
# 5. MPICH bind-to-core for consistent performance
#===============================================================================

#PBS -N adr_scaling_fixed
#PBS -l select=1:ncpus=28
#PBS -l walltime=24:00:00
#PBS -q cpu
#PBS -A Hybrid-Parallelism-ADR-Solver-PDE2025
#PBS -m abe
#PBS -M peng.rao@mail.polimi.it

#===============================================================================
# Setup
#===============================================================================

cd ${PBS_O_WORKDIR:-$(pwd)}

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
LOG_DIR="logs_scaling"
mkdir -p ${LOG_DIR}
LOG_FILE="${LOG_DIR}/scaling_fixed_${PBS_JOBID:-local}_${TIMESTAMP}.log"

RESULTS_DIR="results_scaling_fixed_${TIMESTAMP}"
mkdir -p ${RESULTS_DIR}

exec > >(tee -a "${LOG_FILE}") 2>&1

echo "==============================================================================="
echo "FIXED SCALING BENCHMARK"
echo "==============================================================================="
echo "Job ID: ${PBS_JOBID}"
echo "Date: $(date)"
echo "Results Dir: ${RESULTS_DIR}"
echo ""
echo "KEY FIXES APPLIED:"
echo "  1. Larger problem sizes (ensuring 50k+ DoFs per process)"
echo "  2. Simplified configurations (no redundant overlapping points)"
echo "  3. Scaling metrics based on SOLVE TIME (not total time)"
echo "  4. Pure MPI as primary test, hybrid as separate analysis"
echo "  5. MPICH bind-to-core for consistent performance"
echo "==============================================================================="

#-------------------------------------------------------------------------------
# Spack Environment
#-------------------------------------------------------------------------------
if [ -f "/work/u11022931/spack/share/spack/setup-env.sh" ]; then
    source /work/u11022931/spack/share/spack/setup-env.sh
elif [ -f "/opt/spack/share/spack/setup-env.sh" ]; then
    source /opt/spack/share/spack/setup-env.sh
else
    echo "ERROR: Spack not found"
    exit 1
fi

spack env activate .
echo "Spack Environment: $(spack env status)"
echo ""

ulimit -s unlimited

#===============================================================================
# Build
#===============================================================================

BUILD_DIR="build"
EXECUTABLE="${BUILD_DIR}/scaling_study"
REBUILD_REQUIRED=false

if [ ! -f "$EXECUTABLE" ]; then
    REBUILD_REQUIRED=true
else
    NEWEST_SOURCE=$(find . -maxdepth 2 \( -name "*.cpp" -o -name "*.h" -o -name "CMakeLists.txt" \) 2>/dev/null | xargs ls -t 2>/dev/null | head -n 1)
    if [ -n "$NEWEST_SOURCE" ] && [ "$NEWEST_SOURCE" -nt "$EXECUTABLE" ]; then
        REBUILD_REQUIRED=true
    fi
fi

if [ "$REBUILD_REQUIRED" = true ]; then
    echo "Building scaling benchmark..."
    mkdir -p ${BUILD_DIR}
    cd ${BUILD_DIR}
    cmake .. -DCMAKE_BUILD_TYPE=Release
    make -j 28 scaling_study || exit 1
    cd ..
    echo "Build complete."
else
    echo "Binary is up to date."
fi

#===============================================================================
# Benchmark Parameters
#===============================================================================

DIM=2
DEGREE=2
WARMUP=1
TRIALS=3

# FIXED: Use much larger refinement levels!
# For 2D Q2 elements:
#   refs=8  -> ~263k DoFs   (OK for 1-4 cores)
#   refs=9  -> ~1.05M DoFs  (OK for 1-14 cores)
#   refs=10 -> ~4.2M DoFs   (OK for 1-28 cores)
#   refs=11 -> ~16.8M DoFs  (OK for 28-112 cores)
#
# Minimum: 50,000 DoFs per MPI process for meaningful scaling

MIN_REF_STRONG=9   # Start from ~1M DoFs
MAX_REF_STRONG=11  # Up to ~17M DoFs

MIN_REF_WEAK=7     # Base for weak scaling
MAX_REF_WEAK=9

MPI_CMD="mpirun"

#===============================================================================
# MPICH Binding Configuration
#===============================================================================
# For pure MPI (1 thread per process): bind each process to a core
# For hybrid (multiple threads): bind to none so threads can spread

# Pure MPI binding: each MPI process gets one core
MPI_BIND_CORE="-bind-to core"

# Hybrid binding: don't bind to single core (threads need room)
MPI_BIND_NONE="-bind-to none"

echo ""
echo "==============================================================================="
echo "MPICH Process Binding Configuration"
echo "==============================================================================="
echo "Pure MPI runs:  ${MPI_BIND_CORE}"
echo "Hybrid runs:    ${MPI_BIND_NONE}"
echo "==============================================================================="

echo ""
echo "==============================================================================="
echo "Problem Size Estimates (2D Q2 elements)"
echo "==============================================================================="
echo "refs=8:  ~263,000 DoFs   -> min 5 processes (50k/proc)"
echo "refs=9:  ~1,050,000 DoFs -> min 21 processes (50k/proc)"
echo "refs=10: ~4,200,000 DoFs -> min 84 processes (50k/proc)"
echo "refs=11: ~16,800,000 DoFs -> OK for any config on 28 cores"
echo ""
echo "Selected range: refs ${MIN_REF_STRONG}-${MAX_REF_STRONG}"
echo "==============================================================================="

#===============================================================================
# PHASE 1: PURE MPI STRONG SCALING (Primary Test)
#===============================================================================
# This gives the cleanest scaling curves with one point per core count

echo ""
echo "==============================================================================="
echo "PHASE 1: PURE MPI STRONG SCALING"
echo "==============================================================================="
echo "Testing with 1 thread per MPI process (cleanest scaling curves)"
echo "Using: ${MPI_BIND_CORE}"
echo ""

# Pure MPI configurations (1 thread each) - logarithmic scaling
declare -a PURE_MPI_CONFIGS=(
    "1 1"
    "2 1"
    "4 1"
    "7 1"
    "14 1"
    "28 1"
)

for CONFIG in "${PURE_MPI_CONFIGS[@]}"; do
    read -r NRANKS NTHREADS <<< "$CONFIG"
    TOTAL_CORES=$((NRANKS * NTHREADS))
    CONFIG_NAME="pure_mpi_${NRANKS}"

    echo ""
    echo "-----------------------------------------------------------------------"
    echo "Pure MPI: ${NRANKS} processes × ${NTHREADS} thread = ${TOTAL_CORES} cores"
    echo "Binding: ${MPI_BIND_CORE}"
    echo "-----------------------------------------------------------------------"

    export TBB_NUM_THREADS=${NTHREADS}
    export OMP_NUM_THREADS=${NTHREADS}
    export OMP_PROC_BIND=close
    export OMP_PLACES=cores

    OUTPUT_PREFIX="${RESULTS_DIR}/strong_${CONFIG_NAME}"

    ${MPI_CMD} -n ${NRANKS} ${MPI_BIND_CORE} \
        ${EXECUTABLE} \
        --strong \
        --min-ref ${MIN_REF_STRONG} \
        --max-ref ${MAX_REF_STRONG} \
        --degree ${DEGREE} \
        --dim ${DIM} \
        --output "${OUTPUT_PREFIX}" \
        --threads ${NTHREADS} \
        --warmup ${WARMUP} \
        --trials ${TRIALS} \
        2>&1 | tee "${OUTPUT_PREFIX}.log"

    sleep 2
done

#===============================================================================
# PHASE 2: PURE THREADING SCALING (Secondary Test)
#===============================================================================
# Compare pure threading vs pure MPI

echo ""
echo "==============================================================================="
echo "PHASE 2: PURE THREADING SCALING"
echo "==============================================================================="
echo "Testing with 1 MPI process, varying threads"
echo "Using: ${MPI_BIND_NONE} (threads need multiple cores)"
echo ""

declare -a PURE_THREAD_CONFIGS=(
    "1 1"
    "1 2"
    "1 4"
    "1 7"
    "1 14"
    "1 28"
)

for CONFIG in "${PURE_THREAD_CONFIGS[@]}"; do
    read -r NRANKS NTHREADS <<< "$CONFIG"
    TOTAL_CORES=$((NRANKS * NTHREADS))
    CONFIG_NAME="pure_thread_${NTHREADS}"

    # Skip 1x1 (already done in pure MPI)
    if [ "${NRANKS}" -eq 1 ] && [ "${NTHREADS}" -eq 1 ]; then
        continue
    fi

    echo ""
    echo "-----------------------------------------------------------------------"
    echo "Pure Threading: ${NRANKS} process × ${NTHREADS} threads = ${TOTAL_CORES} cores"
    echo "Binding: ${MPI_BIND_NONE}"
    echo "-----------------------------------------------------------------------"

    export TBB_NUM_THREADS=${NTHREADS}
    export OMP_NUM_THREADS=${NTHREADS}
    export OMP_PROC_BIND=spread
    export OMP_PLACES=cores

    OUTPUT_PREFIX="${RESULTS_DIR}/strong_${CONFIG_NAME}"

    ${MPI_CMD} -n ${NRANKS} ${MPI_BIND_NONE} \
        ${EXECUTABLE} \
        --strong \
        --min-ref ${MIN_REF_STRONG} \
        --max-ref ${MAX_REF_STRONG} \
        --degree ${DEGREE} \
        --dim ${DIM} \
        --output "${OUTPUT_PREFIX}" \
        --threads ${NTHREADS} \
        --warmup ${WARMUP} \
        --trials ${TRIALS} \
        2>&1 | tee "${OUTPUT_PREFIX}.log"

    sleep 2
done

#===============================================================================
# PHASE 3: HYBRID CONFIGURATIONS (Comparison at Fixed Total Cores)
#===============================================================================
# Test different MPI×Thread configurations at specific total core counts

echo ""
echo "==============================================================================="
echo "PHASE 3: HYBRID ANALYSIS AT FIXED CORE COUNTS"
echo "==============================================================================="
echo "Comparing different MPI×Thread configurations at 14 and 28 total cores"
echo "Using: ${MPI_BIND_NONE} (hybrid needs flexible binding)"
echo ""

# 14 total cores: compare different decompositions
declare -a HYBRID_14=(
    "14 1"   # Pure MPI
    "7 2"    # Balanced hybrid
    "2 7"    # Thread-heavy
    "1 14"   # Pure threading
)

# 28 total cores: compare different decompositions
declare -a HYBRID_28=(
    "28 1"   # Pure MPI
    "14 2"   # Balanced hybrid
    "7 4"    # Balanced hybrid
    "4 7"    # Thread-heavy
    "2 14"   # Thread-heavy
    "1 28"   # Pure threading
)

echo "--- Testing 14 cores ---"
for CONFIG in "${HYBRID_14[@]}"; do
    read -r NRANKS NTHREADS <<< "$CONFIG"
    CONFIG_NAME="hybrid_${NRANKS}x${NTHREADS}"

    echo "  ${NRANKS} MPI × ${NTHREADS} threads"

    export TBB_NUM_THREADS=${NTHREADS}
    export OMP_NUM_THREADS=${NTHREADS}
    export OMP_PROC_BIND=close
    export OMP_PLACES=cores

    OUTPUT_PREFIX="${RESULTS_DIR}/hybrid14_${CONFIG_NAME}"

    # Use bind-to-core only for pure MPI (1 thread), otherwise no binding
    if [ "${NTHREADS}" -eq 1 ]; then
        BIND_OPT="${MPI_BIND_CORE}"
    else
        BIND_OPT="${MPI_BIND_NONE}"
    fi

    ${MPI_CMD} -n ${NRANKS} ${BIND_OPT} \
        ${EXECUTABLE} \
        --strong \
        --min-ref ${MAX_REF_STRONG} \
        --max-ref ${MAX_REF_STRONG} \
        --degree ${DEGREE} \
        --dim ${DIM} \
        --output "${OUTPUT_PREFIX}" \
        --threads ${NTHREADS} \
        --warmup ${WARMUP} \
        --trials ${TRIALS} \
        2>&1 > "${OUTPUT_PREFIX}.log"

    sleep 1
done

echo ""
echo "--- Testing 28 cores ---"
for CONFIG in "${HYBRID_28[@]}"; do
    read -r NRANKS NTHREADS <<< "$CONFIG"
    CONFIG_NAME="hybrid_${NRANKS}x${NTHREADS}"

    echo "  ${NRANKS} MPI × ${NTHREADS} threads"

    export TBB_NUM_THREADS=${NTHREADS}
    export OMP_NUM_THREADS=${NTHREADS}
    export OMP_PROC_BIND=close
    export OMP_PLACES=cores

    OUTPUT_PREFIX="${RESULTS_DIR}/hybrid28_${CONFIG_NAME}"

    # Use bind-to-core only for pure MPI (1 thread), otherwise no binding
    if [ "${NTHREADS}" -eq 1 ]; then
        BIND_OPT="${MPI_BIND_CORE}"
    else
        BIND_OPT="${MPI_BIND_NONE}"
    fi

    ${MPI_CMD} -n ${NRANKS} ${BIND_OPT} \
        ${EXECUTABLE} \
        --strong \
        --min-ref ${MAX_REF_STRONG} \
        --max-ref ${MAX_REF_STRONG} \
        --degree ${DEGREE} \
        --dim ${DIM} \
        --output "${OUTPUT_PREFIX}" \
        --threads ${NTHREADS} \
        --warmup ${WARMUP} \
        --trials ${TRIALS} \
        2>&1 > "${OUTPUT_PREFIX}.log"

    sleep 1
done

#===============================================================================
# PHASE 4: WEAK SCALING
#===============================================================================

echo ""
echo "==============================================================================="
echo "PHASE 4: WEAK SCALING (Pure MPI)"
echo "==============================================================================="
echo "Problem size scales with core count to maintain constant work per core"
echo "Using: ${MPI_BIND_CORE}"
echo ""

for CONFIG in "${PURE_MPI_CONFIGS[@]}"; do
    read -r NRANKS NTHREADS <<< "$CONFIG"
    TOTAL_CORES=$((NRANKS * NTHREADS))
    CONFIG_NAME="weak_mpi_${NRANKS}"

    echo ""
    echo "-----------------------------------------------------------------------"
    echo "Weak Scaling: ${NRANKS} processes"
    echo "Binding: ${MPI_BIND_CORE}"
    echo "-----------------------------------------------------------------------"

    export TBB_NUM_THREADS=${NTHREADS}
    export OMP_NUM_THREADS=${NTHREADS}
    export OMP_PROC_BIND=close
    export OMP_PLACES=cores

    OUTPUT_PREFIX="${RESULTS_DIR}/${CONFIG_NAME}"

    ${MPI_CMD} -n ${NRANKS} ${MPI_BIND_CORE} \
        ${EXECUTABLE} \
        --weak \
        --min-ref ${MIN_REF_WEAK} \
        --max-ref ${MAX_REF_WEAK} \
        --degree ${DEGREE} \
        --dim ${DIM} \
        --output "${OUTPUT_PREFIX}" \
        --threads ${NTHREADS} \
        --warmup ${WARMUP} \
        --trials ${TRIALS} \
        2>&1 | tee "${OUTPUT_PREFIX}.log"

    sleep 2
done

#===============================================================================
# Combine Results
#===============================================================================

echo ""
echo "==============================================================================="
echo "Combining Results"
echo "==============================================================================="

# Combine by category
COMBINED_PURE_MPI="${RESULTS_DIR}/combined_pure_mpi.csv"
COMBINED_PURE_THREAD="${RESULTS_DIR}/combined_pure_thread.csv"
COMBINED_HYBRID="${RESULTS_DIR}/combined_hybrid.csv"
COMBINED_WEAK="${RESULTS_DIR}/combined_weak.csv"
COMBINED_ALL="${RESULTS_DIR}/combined_all_results.csv"

# Function to combine CSV files
combine_csv() {
    local output=$1
    shift
    local pattern=$1
    
    first_file=true
    for f in ${RESULTS_DIR}/${pattern}*.csv; do
        if [ -f "$f" ]; then
            if [ "$first_file" = true ]; then
                cat "$f" > ${output}
                first_file=false
            else
                tail -n +2 "$f" >> ${output}
            fi
        fi
    done
    
    if [ -f "${output}" ]; then
        echo "Created: ${output}"
    fi
}

combine_csv "${COMBINED_PURE_MPI}" "strong_pure_mpi_"
combine_csv "${COMBINED_PURE_THREAD}" "strong_pure_thread_"
combine_csv "${COMBINED_HYBRID}" "hybrid"
combine_csv "${COMBINED_WEAK}" "weak_"

# Combine all
first_file=true
for f in "${COMBINED_PURE_MPI}" "${COMBINED_PURE_THREAD}" "${COMBINED_HYBRID}" "${COMBINED_WEAK}"; do
    if [ -f "$f" ]; then
        if [ "$first_file" = true ]; then
            cat "$f" > ${COMBINED_ALL}
            first_file=false
        else
            tail -n +2 "$f" >> ${COMBINED_ALL}
        fi
    fi
done
echo "Created: ${COMBINED_ALL}"

#===============================================================================
# Generate Analysis Script
#===============================================================================

ANALYSIS_SCRIPT="${RESULTS_DIR}/analyze_scaling_fixed.py"

cat > ${ANALYSIS_SCRIPT} << 'PYTHON_EOF'
#!/usr/bin/env python3
"""
Fixed Scaling Analysis Script
- Uses SOLVE TIME for scaling metrics
- Properly handles multiple configurations
- Generates clean, publication-quality plots
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import sys
import os

plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.dpi'] = 150
plt.rcParams['font.size'] = 10

def load_data(filename):
    return pd.read_csv(filename)

def plot_strong_scaling_pure_mpi(df, output_dir):
    """Plot strong scaling using pure MPI data only (cleanest curves)."""
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # Filter: strong scaling, pure MPI (n_threads == 1)
    strong_df = df[(df['test_type'] == 'strong_scaling') & (df['n_threads'] == 1)].copy()
    
    if strong_df.empty:
        print("No pure MPI strong scaling data")
        return
    
    colors = {'matrix_based': 'tab:blue', 'matrix_free': 'tab:orange'}
    markers = {'matrix_based': 'o', 'matrix_free': 's'}
    
    for solver in ['matrix_based', 'matrix_free']:
        solver_df = strong_df[strong_df['solver_type'] == solver]
        if solver_df.empty:
            continue
            
        refs_list = sorted(solver_df['n_refinements'].unique())
        
        for refs in refs_list:
            ref_df = solver_df[solver_df['n_refinements'] == refs].sort_values('n_mpi')
            if len(ref_df) < 2:
                continue
            
            label = f"{solver.replace('_', '-')} (refs={refs})"
            color = colors[solver]
            marker = markers[solver]
            alpha = 0.4 + 0.2 * (refs - min(refs_list))
            
            cores = ref_df['n_mpi'].values
            
            # Compute speedup from SOLVE TIME
            baseline_solve = ref_df['solve_time_avg'].iloc[0]
            speedup = baseline_solve / ref_df['solve_time_avg']
            efficiency = speedup / (cores / cores[0])
            throughput = ref_df['n_dofs'].iloc[0] / ref_df['solve_time_avg']
            
            axes[0, 0].plot(cores, ref_df['solve_time_avg'], f'{marker}-', 
                          label=label, color=color, alpha=alpha, markersize=6)
            axes[0, 1].plot(cores, speedup, f'{marker}-', 
                          label=label, color=color, alpha=alpha, markersize=6)
            axes[1, 0].plot(cores, efficiency, f'{marker}-', 
                          label=label, color=color, alpha=alpha, markersize=6)
            axes[1, 1].plot(cores, throughput, f'{marker}-', 
                          label=label, color=color, alpha=alpha, markersize=6)
    
    # Ideal lines
    max_cores = strong_df['n_mpi'].max()
    axes[0, 1].plot([1, max_cores], [1, max_cores], 'k--', lw=2, label='Ideal')
    axes[1, 0].axhline(y=1.0, color='k', linestyle='--', lw=2, label='Ideal (100%)')
    
    axes[0, 0].set_xlabel('MPI Processes')
    axes[0, 0].set_ylabel('Solve Time (s)')
    axes[0, 0].set_title('Strong Scaling: Solve Time\n(Pure MPI, 1 thread/process)')
    axes[0, 0].legend(fontsize=8)
    axes[0, 0].set_xscale('log', base=2)
    axes[0, 0].set_yscale('log')
    
    axes[0, 1].set_xlabel('MPI Processes')
    axes[0, 1].set_ylabel('Speedup')
    axes[0, 1].set_title('Strong Scaling: Speedup')
    axes[0, 1].legend(fontsize=8)
    axes[0, 1].set_xscale('log', base=2)
    axes[0, 1].set_yscale('log', base=2)
    
    axes[1, 0].set_xlabel('MPI Processes')
    axes[1, 0].set_ylabel('Parallel Efficiency')
    axes[1, 0].set_title('Strong Scaling: Efficiency')
    axes[1, 0].legend(fontsize=8)
    axes[1, 0].set_xscale('log', base=2)
    axes[1, 0].set_ylim([0, 1.2])
    
    axes[1, 1].set_xlabel('MPI Processes')
    axes[1, 1].set_ylabel('DoFs/second')
    axes[1, 1].set_title('Strong Scaling: Throughput')
    axes[1, 1].legend(fontsize=8)
    axes[1, 1].set_xscale('log', base=2)
    axes[1, 1].set_yscale('log')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'strong_scaling_pure_mpi.png'))
    plt.close()
    print("Saved: strong_scaling_pure_mpi.png")

def plot_mpi_vs_threading(df, output_dir):
    """Compare pure MPI vs pure threading scaling."""
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    strong_df = df[df['test_type'] == 'strong_scaling'].copy()
    if strong_df.empty:
        return
    
    # Get largest refinement level
    max_refs = strong_df['n_refinements'].max()
    strong_df = strong_df[strong_df['n_refinements'] == max_refs]
    
    # Pure MPI: n_threads == 1
    # Pure threading: n_mpi == 1
    
    for solver in ['matrix_based', 'matrix_free']:
        color = 'tab:blue' if solver == 'matrix_based' else 'tab:orange'
        label_base = solver.replace('_', '-')
        
        # Pure MPI
        mpi_df = strong_df[(strong_df['solver_type'] == solver) & 
                           (strong_df['n_threads'] == 1)].sort_values('total_cores')
        
        # Pure threading
        thr_df = strong_df[(strong_df['solver_type'] == solver) & 
                           (strong_df['n_mpi'] == 1)].sort_values('total_cores')
        
        if not mpi_df.empty:
            baseline = mpi_df['solve_time_avg'].iloc[0]
            speedup = baseline / mpi_df['solve_time_avg']
            axes[0].plot(mpi_df['total_cores'], speedup, 'o-', 
                        color=color, label=f'{label_base} (MPI)')
        
        if not thr_df.empty:
            baseline = thr_df['solve_time_avg'].iloc[0]
            speedup = baseline / thr_df['solve_time_avg']
            axes[0].plot(thr_df['total_cores'], speedup, 's--', 
                        color=color, label=f'{label_base} (Threading)', alpha=0.7)
        
        # Time comparison
        if not mpi_df.empty:
            axes[1].plot(mpi_df['total_cores'], mpi_df['solve_time_avg'], 'o-',
                        color=color, label=f'{label_base} (MPI)')
        if not thr_df.empty:
            axes[1].plot(thr_df['total_cores'], thr_df['solve_time_avg'], 's--',
                        color=color, label=f'{label_base} (Threading)', alpha=0.7)
    
    # Ideal
    max_cores = strong_df['total_cores'].max()
    axes[0].plot([1, max_cores], [1, max_cores], 'k--', lw=2, label='Ideal')
    
    axes[0].set_xlabel('Total Cores')
    axes[0].set_ylabel('Speedup')
    axes[0].set_title(f'MPI vs Threading: Speedup\n(refs={max_refs})')
    axes[0].legend(fontsize=8)
    axes[0].set_xscale('log', base=2)
    axes[0].set_yscale('log', base=2)
    
    axes[1].set_xlabel('Total Cores')
    axes[1].set_ylabel('Solve Time (s)')
    axes[1].set_title(f'MPI vs Threading: Solve Time\n(refs={max_refs})')
    axes[1].legend(fontsize=8)
    axes[1].set_xscale('log', base=2)
    axes[1].set_yscale('log')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'mpi_vs_threading.png'))
    plt.close()
    print("Saved: mpi_vs_threading.png")

def plot_hybrid_comparison(df, output_dir):
    """Compare different MPI×Thread configurations at same total core count."""
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    strong_df = df[df['test_type'] == 'strong_scaling'].copy()
    if strong_df.empty:
        return
    
    # Get configs at 14 and 28 cores
    for ax_idx, target_cores in enumerate([14, 28]):
        ax = axes[ax_idx]
        
        core_df = strong_df[strong_df['total_cores'] == target_cores]
        if core_df.empty:
            continue
        
        # Get largest refs available
        max_refs = core_df['n_refinements'].max()
        core_df = core_df[core_df['n_refinements'] == max_refs]
        
        for solver in ['matrix_based', 'matrix_free']:
            solver_df = core_df[core_df['solver_type'] == solver].copy()
            if solver_df.empty:
                continue
            
            # Create config labels
            solver_df['config'] = solver_df.apply(
                lambda r: f"{int(r['n_mpi'])}M×{int(r['n_threads'])}T", axis=1)
            solver_df = solver_df.sort_values('n_mpi', ascending=False)
            
            x = np.arange(len(solver_df))
            width = 0.35
            offset = -width/2 if solver == 'matrix_based' else width/2
            
            color = 'tab:blue' if solver == 'matrix_based' else 'tab:orange'
            ax.bar(x + offset, solver_df['solve_time_avg'], width,
                  label=solver.replace('_', '-'), color=color)
            
            if solver == 'matrix_based':
                ax.set_xticks(x)
                ax.set_xticklabels(solver_df['config'].values, rotation=45, ha='right')
        
        ax.set_xlabel('Configuration (MPI × Threads)')
        ax.set_ylabel('Solve Time (s)')
        ax.set_title(f'Hybrid Comparison: {target_cores} Total Cores\n(refs={max_refs})')
        ax.legend()
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'hybrid_comparison.png'))
    plt.close()
    print("Saved: hybrid_comparison.png")

def plot_weak_scaling(df, output_dir):
    """Plot weak scaling results."""
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    weak_df = df[df['test_type'] == 'weak_scaling'].copy()
    if weak_df.empty:
        print("No weak scaling data")
        return
    
    # Use pure MPI only
    weak_df = weak_df[weak_df['n_threads'] == 1]
    
    for solver in ['matrix_based', 'matrix_free']:
        solver_df = weak_df[weak_df['solver_type'] == solver].sort_values('n_mpi')
        if solver_df.empty:
            continue
        
        color = 'tab:blue' if solver == 'matrix_based' else 'tab:orange'
        marker = 'o' if solver == 'matrix_based' else 's'
        label = solver.replace('_', '-')
        
        # Weak scaling: time should stay constant
        axes[0].plot(solver_df['n_mpi'], solver_df['solve_time_avg'], 
                    f'{marker}-', label=label, color=color, markersize=8)
        
        # Efficiency = T(1) / T(p)
        if len(solver_df) > 0:
            baseline = solver_df['solve_time_avg'].iloc[0]
            efficiency = baseline / solver_df['solve_time_avg']
            axes[1].plot(solver_df['n_mpi'], efficiency,
                        f'{marker}-', label=label, color=color, markersize=8)
    
    axes[0].set_xlabel('MPI Processes')
    axes[0].set_ylabel('Solve Time (s)')
    axes[0].set_title('Weak Scaling: Solve Time\n(should be constant for ideal scaling)')
    axes[0].legend()
    axes[0].set_xscale('log', base=2)
    
    axes[1].axhline(y=1.0, color='k', linestyle='--', lw=2, label='Ideal')
    axes[1].set_xlabel('MPI Processes')
    axes[1].set_ylabel('Efficiency = T(1)/T(p)')
    axes[1].set_title('Weak Scaling: Efficiency')
    axes[1].legend()
    axes[1].set_xscale('log', base=2)
    axes[1].set_ylim([0, 1.5])
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'weak_scaling.png'))
    plt.close()
    print("Saved: weak_scaling.png")

def print_summary(df):
    """Print data summary."""
    print("\n" + "="*70)
    print("DATA SUMMARY")
    print("="*70)
    print(f"Total records: {len(df)}")
    print(f"Solvers: {df['solver_type'].unique().tolist()}")
    print(f"Test types: {df['test_type'].unique().tolist()}")
    
    if 'dofs_per_process' in df.columns:
        print(f"\nDoFs per process range: {df['dofs_per_process'].min():,} - {df['dofs_per_process'].max():,}")
        low_dofs = df[df['dofs_per_process'] < 50000]
        if len(low_dofs) > 0:
            print(f"WARNING: {len(low_dofs)} runs have < 50k DoFs/process")
    
    print("\nStrong scaling (pure MPI) summary:")
    strong_mpi = df[(df['test_type'] == 'strong_scaling') & (df['n_threads'] == 1)]
    for solver in strong_mpi['solver_type'].unique():
        solver_df = strong_mpi[strong_mpi['solver_type'] == solver]
        for refs in sorted(solver_df['n_refinements'].unique()):
            ref_df = solver_df[solver_df['n_refinements'] == refs].sort_values('n_mpi')
            if len(ref_df) >= 2:
                t1 = ref_df['solve_time_avg'].iloc[0]
                tn = ref_df['solve_time_avg'].iloc[-1]
                n1 = ref_df['n_mpi'].iloc[0]
                nn = ref_df['n_mpi'].iloc[-1]
                speedup = t1 / tn
                ideal = nn / n1
                eff = speedup / ideal * 100
                print(f"  {solver} refs={refs}: T({n1})={t1:.3f}s -> T({nn})={tn:.3f}s, "
                      f"Speedup={speedup:.1f}x (ideal={ideal:.0f}x), Eff={eff:.0f}%")

def main():
    if len(sys.argv) < 2:
        print("Usage: python analyze_scaling_fixed.py <results_csv>")
        sys.exit(1)
    
    csv_file = sys.argv[1]
    output_dir = os.path.dirname(csv_file) or '.'
    
    print(f"Loading: {csv_file}")
    df = load_data(csv_file)
    
    print_summary(df)
    
    print("\nGenerating plots...")
    plot_strong_scaling_pure_mpi(df, output_dir)
    plot_mpi_vs_threading(df, output_dir)
    plot_hybrid_comparison(df, output_dir)
    plot_weak_scaling(df, output_dir)
    
    print("\nDone!")

if __name__ == "__main__":
    main()
PYTHON_EOF

chmod +x ${ANALYSIS_SCRIPT}

#===============================================================================
# Final Summary
#===============================================================================

echo ""
echo "==============================================================================="
echo "BENCHMARK COMPLETE"
echo "==============================================================================="
echo "Results: ${RESULTS_DIR}"
echo ""
echo "Key output files:"
echo "  ${COMBINED_PURE_MPI}     - Pure MPI scaling (primary)"
echo "  ${COMBINED_PURE_THREAD}  - Pure threading scaling"
echo "  ${COMBINED_HYBRID}       - Hybrid configurations"
echo "  ${COMBINED_WEAK}         - Weak scaling"
echo "  ${COMBINED_ALL}          - All results combined"
echo ""
echo "To generate plots:"
echo "  python ${ANALYSIS_SCRIPT} ${COMBINED_ALL}"
echo ""
echo "MPICH binding used:"
echo "  Pure MPI:  ${MPI_BIND_CORE}"
echo "  Hybrid:    ${MPI_BIND_NONE}"
echo ""
echo "End: $(date)"
echo "==============================================================================="
