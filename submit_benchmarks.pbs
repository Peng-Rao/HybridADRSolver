#!/bin/bash
#===============================================================================
# PBS Job Script for Hybrid ADR Solver Benchmarks
#===============================================================================
# Cluster Configuration: 5 nodes × 112 cores/node = 560 total cores
# Threading: TBB (deal.II matrix-free) - optimized for fewer ranks, more threads
# Submit with: qsub submit_benchmarks.pbs
# Check status: qstat -u $USER
# Delete job: qdel <job_id>
#===============================================================================

#-------------------------------------------------------------------------------
# PBS Directives
#-------------------------------------------------------------------------------
#PBS -N adr_benchmark
#PBS -l select=4:ncpus=28:mpiprocs=1:ompthreads=28
#PBS -l walltime=04:00:00
#PBS -q cpu
#PBS -A Hybrid-Parallelism-ADR-Solver-PDE2025
#PBS -m abe
#PBS -M peng.rao@mail.polimi.it

#-------------------------------------------------------------------------------
# Resource Explanation (5 nodes × 112 cores/node cluster):
#   select=4          : Number of nodes (max 5 available)
#   ncpus=112         : CPUs per node (112 cores/node)
#   mpiprocs=4        : MPI ranks per node
#   ompthreads=28     : Threads per MPI rank (4 × 28 = 112)
#   
# TBB-optimized configurations (fewer ranks = better for TBB work-stealing):
#   mpiprocs=2,  ompthreads=56   (2 ranks × 56 threads) - maximum threads
#   mpiprocs=4,  ompthreads=28   (4 ranks × 28 threads) <- RECOMMENDED for TBB
#   mpiprocs=7,  ompthreads=16   (7 ranks × 16 threads)
#   mpiprocs=8,  ompthreads=14   (8 ranks × 14 threads)
#   mpiprocs=14, ompthreads=8    (14 ranks × 8 threads) - more MPI overhead
#-------------------------------------------------------------------------------

#===============================================================================
# Setup Logging and Results Directories
#===============================================================================

# Move to submission directory
cd ${PBS_O_WORKDIR:-$(pwd)}

# Create timestamp for this run
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# Create log directory and file
LOG_DIR="logs"
mkdir -p ${LOG_DIR}
LOG_FILE="${LOG_DIR}/benchmark_${PBS_JOBID:-local}_${TIMESTAMP}.log"

# Create results directory
RESULTS_DIR="results_${TIMESTAMP}"
mkdir -p ${RESULTS_DIR}

# Redirect all output to log file while also displaying to stdout
exec > >(tee -a "${LOG_FILE}") 2>&1

# Log system info
echo "==============================================================================="
echo "PBS Job Information"
echo "==============================================================================="
echo "Job ID:        ${PBS_JOBID}"
echo "Job Name:      ${PBS_JOBNAME}"
echo "Queue:         ${PBS_QUEUE}"
echo "Nodes File:    ${PBS_NODEFILE}"
echo "Work Dir:      ${PBS_O_WORKDIR}"
echo "Results Dir:   ${RESULTS_DIR}"
echo "Log File:      ${LOG_FILE}"
echo "Date:          $(date)"
echo "==============================================================================="

#-------------------------------------------------------------------------------
# Spack Environment Setup
#-------------------------------------------------------------------------------
# Source Spack from user's installation
if [ -f "/work/u11022931/spack/share/spack/setup-env.sh" ]; then
    source /work/u11022931/spack/share/spack/setup-env.sh
elif [ -f "/opt/spack/share/spack/setup-env.sh" ]; then
    source /opt/spack/share/spack/setup-env.sh
else
    echo "ERROR: Spack not found. Please update the path."
    exit 1
fi

# Activate the Spack environment (directory-based)
spack env activate .

# Verify environment is loaded
echo ""
echo "Spack Environment: $(spack env status)"
echo ""
echo "Loaded Packages:"
spack find --loaded
echo ""

#-------------------------------------------------------------------------------
# Set Environment Variables
#-------------------------------------------------------------------------------

# TBB Threading Configuration (primary for deal.II matrix-free)
# TBB uses work-stealing, so more threads per rank is beneficial
export TBB_NUM_THREADS=${TBB_NUM_THREADS:-28}

# OpenMP settings (for compatibility, TBB is primary)
export OMP_NUM_THREADS=${TBB_NUM_THREADS}
export OMP_PROC_BIND=close
export OMP_PLACES=cores
export OMP_STACKSIZE=64M

# Intel MPI settings (if using Intel MPI)
export I_MPI_PIN=1
export I_MPI_PIN_DOMAIN=omp
export I_MPI_PIN_ORDER=scatter

# Memory settings
ulimit -s unlimited

# Calculate total resources
NNODES=$(cat ${PBS_NODEFILE} | sort -u | wc -l)
NRANKS_PER_NODE=${PBS_NUM_PPN:-4}
TOTAL_RANKS=$((NNODES * NRANKS_PER_NODE))
THREADS_PER_RANK=${TBB_NUM_THREADS}
TOTAL_CORES=$((TOTAL_RANKS * THREADS_PER_RANK))

echo "==============================================================================="
echo "Resource Configuration (TBB Threading)"
echo "==============================================================================="
echo "Nodes:              ${NNODES}"
echo "MPI Ranks/Node:     ${NRANKS_PER_NODE}"
echo "Total MPI Ranks:    ${TOTAL_RANKS}"
echo "TBB Threads/Rank:   ${THREADS_PER_RANK}"
echo "Total Parallelism:  ${TOTAL_CORES}"
echo "==============================================================================="
echo ""

#===============================================================================
# Build
#===============================================================================

BUILD_DIR="build"
EXECUTABLE="${BUILD_DIR}/benchmark_strong_scaling"
REBUILD_REQUIRED=false

if [ ! -f "$EXECUTABLE" ]; then
    echo "EXE: Executable not found. Building..."
    REBUILD_REQUIRED=true
else
    # Check if any .cpp or .h files in the current dir/include/src are newer than the binary
    # This ensures that if you change the code, the script detects it.
    NEWEST_SOURCE=$(find . -maxdepth 2 -name "*.cpp" -o -name "*.h" -o -name "CMakeLists.txt" | xargs ls -t | head -n 1)
    if [ "$NEWEST_SOURCE" -nt "$EXECUTABLE" ]; then
        echo "EXE: Source code change detected ($NEWEST_SOURCE is newer than binary). Rebuilding..."
        REBUILD_REQUIRED=true
    fi
fi

if [ "$REBUILD_REQUIRED" = true ]; then
    mkdir -p ${BUILD_DIR}
    cd ${BUILD_DIR}
    cmake .. -DCMAKE_BUILD_TYPE=Release
    make -j ${NCPUS} || exit 1  # Use all available cores for fast build
    cd ..
    echo "EXE: Build complete."
else
    echo "EXE: Binary is up to date. Skipping build."
fi

#===============================================================================
# Run Benchmarks
#===============================================================================

# Auto-detect MPI implementation and set appropriate options
MPI_CMD="mpirun"

if ${MPI_CMD} --version 2>&1 | grep -q "Open MPI"; then
    echo "Detected: OpenMPI"
    MPI_MAP_OPTS="--map-by ppr:${NRANKS_PER_NODE}:node:PE=${THREADS_PER_RANK} --bind-to core"
elif ${MPI_CMD} --version 2>&1 | grep -q "HYDRA\|MPICH"; then
    echo "Detected: MPICH/HYDRA"
    MPI_MAP_OPTS="-ppn ${NRANKS_PER_NODE} -bind-to core"
elif ${MPI_CMD} --version 2>&1 | grep -q "Intel"; then
    echo "Detected: Intel MPI"
    MPI_MAP_OPTS="-ppn ${NRANKS_PER_NODE} -genv I_MPI_PIN=1 -genv I_MPI_PIN_DOMAIN=omp"
else
    echo "Unknown MPI implementation - using basic options"
    MPI_MAP_OPTS=""
fi

echo "MPI Command: ${MPI_CMD} -np ${TOTAL_RANKS} ${MPI_MAP_OPTS}"
echo ""

echo "==============================================================================="
echo "Running Strong Scaling Benchmark"
echo "==============================================================================="

for REFINEMENT in 3 4 5; do
    echo ""
    echo "--- Refinement Level: ${REFINEMENT} ---"
    
    ${MPI_CMD} -np ${TOTAL_RANKS} ${MPI_MAP_OPTS} \
        ${BUILD_DIR}/benchmark_strong_scaling ${REFINEMENT} \
        2>&1 | tee ${RESULTS_DIR}/strong_scaling_ref${REFINEMENT}_${TOTAL_RANKS}ranks.log
    
    # Move generated CSV to results
    mv strong_scaling_*.csv ${RESULTS_DIR}/ 2>/dev/null || true
done

echo ""
echo "==============================================================================="
echo "Running Weak Scaling Benchmark"
echo "==============================================================================="

for BASE_REF in 2 3; do
    echo ""
    echo "--- Base Refinement: ${BASE_REF} ---"
    
    ${MPI_CMD} -np ${TOTAL_RANKS} ${MPI_MAP_OPTS} \
        ${BUILD_DIR}/benchmark_weak_scaling ${BASE_REF} \
        2>&1 | tee ${RESULTS_DIR}/weak_scaling_base${BASE_REF}_${TOTAL_RANKS}ranks.log
    
    # Move generated CSV to results
    mv weak_scaling_*.csv ${RESULTS_DIR}/ 2>/dev/null || true
done

echo ""
echo "==============================================================================="
echo "Running Main Benchmark"
echo "==============================================================================="

${MPI_CMD} -np ${TOTAL_RANKS} ${MPI_MAP_OPTS} \
    ${BUILD_DIR}/benchmark_main \
    2>&1 | tee ${RESULTS_DIR}/benchmark_main_${TOTAL_RANKS}ranks.log

# Move any additional output files
mv *.csv ${RESULTS_DIR}/ 2>/dev/null || true
mv *.vtu ${RESULTS_DIR}/ 2>/dev/null || true
mv *.pvtu ${RESULTS_DIR}/ 2>/dev/null || true

#===============================================================================
# Post-processing
#===============================================================================

echo ""
echo "==============================================================================="
echo "Generating Analysis Plots"
echo "==============================================================================="

# Load Python if needed
# module load python/3.10
# module load matplotlib pandas numpy

if command -v python3 &> /dev/null; then
    python3 analyze_results.py \
        --results-dir ${RESULTS_DIR} \
        --output-dir ${RESULTS_DIR}/figures \
        2>&1 | tee ${RESULTS_DIR}/analysis.log
else
    echo "Python not available for analysis. Run analyze_results.py manually."
fi

#===============================================================================
# Cleanup and Summary
#===============================================================================

echo ""
echo "==============================================================================="
echo "Job Complete"
echo "==============================================================================="
echo "Results saved to: ${RESULTS_DIR}"
echo "Log file:         ${LOG_FILE}"
echo "End time: $(date)"
echo ""

# List results
echo "Generated files:"
ls -la ${RESULTS_DIR}/

echo ""
echo "==============================================================================="
