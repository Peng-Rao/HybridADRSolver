\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[textwidth = 460pt,top = 80pt, bottom = 80pt]{geometry}
\usepackage{graphicx}
\usepackage[justification=default]{subfig} 
\usepackage[update]{epstopdf}
\usepackage[labelfont=bf]{caption}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancyhdr}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{titling}
\usepackage{float}
\usepackage{bm}
\usepackage[intlimits]{empheq}
\usepackage[hidelinks]{hyperref}
\usepackage{amsmath}
 \usepackage{amssymb}

\usepackage{tikz}
\usepackage{listings}
\lstset{
    basicstyle=\small\ttfamily,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{gray}\itshape,
    stringstyle=\color{red},
    numbers=none,
    breaklines=true,
    showstringspaces=false,
    tabsize=4
}
\usetikzlibrary{positioning,3d,shapes.geometric, arrows.meta, fit}


%Bibliography

\usepackage{csquotes}
\usepackage[sorting=none,%
sortcites=true,%
bibencoding=ascii,%
autopunct=true,%
hyperref=true,%
language=auto,%
%backref=true,%
url=false,%
%maxcitenames=10,%
%minbibnames = 20,%
maxbibnames = 3,%
giveninits, 
natbib = false,
isbn=false,%
backend=biber]{biblatex}
\addbibresource{references.bib}

\usepackage[]{hyperref}
\usepackage{cleveref}
%%% CREF setup
\crefname{equation}{Eq.}{Eqs.}
\crefname{table}{Table}{Tables}
\crefname{figure}{Fig.}{Figs.}


\begin{document}

\begin{titlingpage}
    \begin{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.6\textwidth]{figures/logopolimi_mod.jpg}
        \end{figure}
        \Large{\textsc{Politecnico di Milano \\ School of Industrial and Information Engineering \\ M.Sc. in High Performance Computing}}

        \vspace{1cm}

        \rule{0.95\textwidth}{0.7mm}
        {\Large{\textbf{Hybrid thread-MPI parallelization for ADR equation}}}
        \rule{0.95\textwidth}{0.7mm}

        \vspace{1cm}

        \large{\textbf{Peng Rao}, \textbf{Jiali Claudio Huang}, \textbf{Ruiying Jiao}}

        \vspace{1cm}

        \small{\textbf{Repo: \href{https://github.com/Peng-Rao/HybridADRSolver}{https://github.com/Peng-Rao/HybridADRSolver}}}
    \end{center}

\end{titlingpage}

\pagenumbering{roman}

\tableofcontents

\clearpage

\setcounter{page}{1}
\pagenumbering{arabic}

\section{Problem statement} \label{sec:problem_statement}
\subsection{Strong formulation}
Consider the following \textbf{Advection-Diffusion-Reaction} equation with mixed Dirichlet-Neumann boundary conditions:
\[
    \begin{cases}
        -\nabla \cdot (\mu \nabla u) + \nabla \cdot (\boldsymbol{\beta} u) + \gamma u = f
         & \text{in } \Omega,                                         \\[0.5em]
        u = g
         & \text{on } \Gamma_D \subset \partial \Omega,               \\[0.5em]
        \nabla u \cdot \boldsymbol{n} = h
         & \text{on } \Gamma_N = \partial \Omega \setminus \Gamma_D .
    \end{cases}
\]
where:
\begin{itemize}
    \item $\Omega \subset \mathbb{R}^d$ (with $d = 1, 2, 3$) is an open bounded domain with boundary $\partial \Omega$;
    \item $\mu > 0$ is the diffusion coefficient;
    \item $\boldsymbol{\beta} \in [L^{\infty}(\Omega)]^d$ is the advection velocity field;
    \item $\gamma \geq 0$ is the reaction coefficient;
    \item $f \in L^2(\Omega)$ is a source term;
    \item $g \in H^{1/2}(\Gamma_D)$ is the Dirichlet boundary data;
    \item $h \in L^2(\Gamma_N)$ is the Neumann boundary data;
    \item $\boldsymbol{n}$ is the outward unit normal vector on the boundary $\partial \Omega$.
    \item $u$ is the unknown scalar function to be solved for.
    \item $\Gamma_D$ and $\Gamma_N$ are the Dirichlet and Neumann parts of the boundary, respectively.
\end{itemize}

\subsection{Weak formulation}
We begin by defining the trial and test function spaces. To accommodate the non-homogeneous Dirichlet boundary condition, we introduce a \textit{lifting function} $u_g \in H^1(\Omega)$ such that $u_g = g$ on $\Gamma_D$:
\[
    V_g := \{ v \in H^1(\Omega) : v = g \quad \text{on } \Gamma_D \}
\]
The test space is the linear subspace of $H^1(\Omega)$ with homogeneous Dirichlet boundary conditions:
\[
    V_0 := \{ v \in H^1(\Omega) : v = 0 \quad \text{on } \Gamma_D \}
\]
Consequently, we decompose the solution as $u = u_0 + u_g$, where the unknown $u_0 \in V_0$.
\
Multiply the equation by a test function $v \in V_0$ and integrate over the domain $\Omega$:
\[
    \int_\Omega \left( -\nabla \cdot (\mu \nabla u) + \nabla \cdot (\boldsymbol{\beta} u) + \gamma u \right) v \, d\Omega = \int_\Omega f v \, d\Omega
\]
Using the linearity of the integral, we separate the terms:
\[
    -\int_\Omega \nabla \cdot (\mu \nabla u) v \, d\Omega + \int_\Omega \nabla \cdot (\boldsymbol{\beta} u) v \, d\Omega + \int_\Omega \gamma u v \, d\Omega = \int_\Omega f v \, d\Omega
\]
We apply Green's first identity to the diffusion term to reduce the order of differentiation:
\[
    -\int_\Omega \nabla \cdot (\mu \nabla u) v \, d\Omega = \int_\Omega \mu \nabla u \cdot \nabla v \, d\Omega - \int_{\partial \Omega} (\mu \nabla u \cdot \mathbf{n}) v \, d\Gamma
\]
Substituting this back into the integral equation:
\[
    \int_\Omega \mu \nabla u \cdot \nabla v \, d\Omega - \int_{\partial \Omega} (\mu \nabla u \cdot \mathbf{n}) v \, d\Gamma + \int_\Omega \nabla \cdot (\boldsymbol{\beta} u) v \, d\Omega + \int_\Omega \gamma u v \, d\Omega = \int_\Omega f v \, d\Omega
\]
Split the boundary integral into contributions from $\Gamma_D$ and $\Gamma_N$:
\[
    \int_{\partial \Omega} (\mu \nabla u \cdot \mathbf{n}) v \, d\Gamma = \int_{\Gamma_D} (\mu \nabla u \cdot \mathbf{n}) v \, d\Gamma + \int_{\Gamma_N} (\mu \nabla u \cdot \mathbf{n}) v \, d\Gamma
\]
Since $v = 0$ on $\Gamma_D$, the first term vanishes. On $\Gamma_N$, we use the Neumann condition $\nabla u \cdot \mathbf{n} = h$:
\[
    \int_{\Gamma_N} (\mu \nabla u \cdot \mathbf{n}) v \, d\Gamma = \int_{\Gamma_N} \mu h v \, d\Gamma
\]
Substituting back, we have:
\[
    \int_\Omega \mu \nabla u \cdot \nabla v \, d\Omega + \int_\Omega \nabla \cdot (\boldsymbol{\beta} u) v \, d\Omega + \int_\Omega \gamma u v \, d\Omega = \int_\Omega f v \, d\Omega + \int_{\Gamma_N} \mu h v \, d\Gamma
\]
Substituting $u = u_0 + u_g$ into the equation, we get:

\noindent\textit{Find $u_0 \in V_0$ such that}
\[
    a(u_0, v) = F(v), \quad \forall v \in V_0
\]
where the \textit{bilinear form} $a: V_0 \times V_0 \to \mathbb{R}$ is defined as:
\[
    a(u, v) = \int_\Omega \mu \nabla u \cdot \nabla v \, d\Omega + \int_\Omega \nabla \cdot (\boldsymbol{\beta} u) v \, d\Omega + \int_\Omega \gamma u v \, d\Omega
\]
and the \textit{linear functional} $F: V_0 \to \mathbb{R}$ is given by:
\[
    F(v) = \int_\Omega f v \, d\Omega + \int_{\Gamma_N} \mu h v \, d\Gamma - a(u_g, v)
\]

\subsection{Manufactured solution}
We define the exact solution $u_{\text{ex}}: \Omega \to \mathbb{R}$ on the unit hypercube domain $\Omega = [0, 1]^d$ (where $d=2, 3$) as the product of sine functions:
\begin{equation} \label{eq:exact_sol}
    u_{\text{ex}}(\mathbf{x}) = \prod_{i=1}^{d} \sin(\pi x_i).
\end{equation}
This function vanishes on the boundary hyperplanes where $x_i = 0$ or $x_i = 1$, making it naturally suitable for homogeneous Dirichlet boundary conditions.

% \subsection{Problem Parameters}
The physical coefficients for the benchmark problem are chosen as follows:
\begin{itemize}
    \item \textbf{Diffusion:} A constant isotropic diffusion coefficient $\mu = 1.0$.
    \item \textbf{Reaction:} A constant reaction coefficient $\gamma = 0.1$.
    \item \textbf{Advection:} A rotational velocity field $\boldsymbol{\beta}(\mathbf{x})$, defined to make the problem non-symmetric:
          \begin{equation}
              \boldsymbol{\beta}(\mathbf{x}) =
              \begin{cases}
                  \begin{bmatrix} -x_2 \\ x_1 \end{bmatrix}        & \text{if } d=2, \\[1.5em]
                  \begin{bmatrix} -x_2 \\ x_1 \\ 0.1 \end{bmatrix} & \text{if } d=3.
              \end{cases}
          \end{equation}
\end{itemize}

% \subsection{Derivation of the Source Term}
Substituting $u_{\text{ex}}$ into the governing equation $-\nabla \cdot (\mu \nabla u) + \nabla \cdot (\boldsymbol{\beta} u) + \gamma u = f$, we compute the source term $f$.

First, we observe that the Laplacian of the chosen exact solution is:
\begin{equation}
    \Delta u_{\text{ex}} = \sum_{i=1}^d \frac{\partial^2 u_{\text{ex}}}{\partial x_i^2} = \sum_{i=1}^d (-\pi^2 u_{\text{ex}}) = -d \pi^2 u_{\text{ex}}.
\end{equation}
Assuming $\boldsymbol{\beta}$ is divergence-free ($\nabla \cdot \boldsymbol{\beta} = 0$, which holds for the rotational field defined above), the advection term simplifies to $\boldsymbol{\beta} \cdot \nabla u_{\text{ex}}$. The source term $f$ is therefore implemented as:
\begin{equation}
    f(\mathbf{x}) = \mu d \pi^2 u_{\text{ex}}(\mathbf{x}) + \boldsymbol{\beta}(\mathbf{x}) \cdot \nabla u_{\text{ex}}(\mathbf{x}) + \gamma u_{\text{ex}}(\mathbf{x}).
\end{equation}

% \subsection{Boundary Conditions}
The problem domain boundary $\partial \Omega$ is split into Dirichlet ($\Gamma_D$) and Neumann ($\Gamma_N$) portions to test mixed boundary conditions. Figure \ref{fig:boundary_conditions} illustrates the boundary conditions on a 2D unit square domain.

\paragraph{Neumann Boundary ($\Gamma_N$)}
We apply a Neumann condition on the ``Right'' face of the hypercube, defined as the plane $x_1 = 1$. The outward unit normal is $\mathbf{n} = (1, 0, \dots)^T$. The required flux $h$ is derived from the exact solution:
\begin{equation}
    h(\mathbf{x}) = \nabla u_{\text{ex}} \cdot \mathbf{n} \Big|_{x_1=1} = \frac{\partial u_{\text{ex}}}{\partial x_1} \Big|_{x_1=1}.
\end{equation}
Computing the partial derivative:
\begin{align}
    \frac{\partial u_{\text{ex}}}{\partial x_1} & = \pi \cos(\pi x_1) \prod_{j=2}^d \sin(\pi x_j).
\end{align}
Evaluated at $x_1 = 1$, where $\cos(\pi) = -1$, the Neumann data imposed is:
\begin{equation}
    h(\mathbf{x}) = -\pi \prod_{j=2}^d \sin(\pi x_j).
\end{equation}

\paragraph{Dirichlet Boundary ($\Gamma_D$)}
On all other boundaries ($\partial \Omega \setminus \Gamma_N$), we enforce a homogeneous Dirichlet condition:
\begin{equation}
    u = 0 \quad \text{on } \Gamma_D.
\end{equation}
This is consistent with the exact solution, as $\sin(\pi x_i) = 0$ when $x_i \in \{0, 1\}$.


\begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=5]
        % Define coordinates for the unit square corners
        \coordinate (O) at (0,0); % Bottom-left
        \coordinate (A) at (1,0); % Bottom-right
        \coordinate (B) at (1,1); % Top-right
        \coordinate (C) at (0,1); % Top-left

        % Draw Axes for context
        \draw[->, thick, gray] (-0.2,0) -- (1.3,0) node[right, black] {$x_1$};
        \draw[->, thick, gray] (0,-0.2) -- (0,1.3) node[above, black] {$x_2$};

        % --- Draw Dirichlet Boundaries ($\Gamma_D$) ---
        % Left ($x_1=0$), Bottom ($x_2=0$), Top ($x_2=1$)
        % Using thick blue lines
        \draw[line width=2.5pt, blue] (C) -- (O) -- (A);
        \draw[line width=2.5pt, blue] (C) -- (B);

        % --- Draw Neumann Boundary ($\Gamma_N$) ---
        % Right face ($x_1=1$)
        % Using thick red line
        \draw[line width=2.5pt, red] (A) -- (B);

        % --- Labels and Annotations ---

        % Domain label
        \node at (0.5, 0.5) {\Huge $\Omega$};

        % Corner coordinate labels
        \node[below left] at (O) {$(0,0)$};
        \node[below] at (A) {$(1,0)$};
        \node[above] at (B) {$(1,1)$};
        \node[left] at (C) {$(0,1)$};

        % Dirichlet boundary labels
        % Labeling the regions
        \node[blue, left, font=\large] at (0, 0.5) {$\Gamma_D$};
        \node[blue, below, font=\large] at (0.5, 0) {$\Gamma_D$};
        \node[blue, above, font=\large] at (0.5, 1) {$\Gamma_D$};
        % Labeling the condition
        \node[blue, fill=white, rounded corners=2pt, inner sep=2pt] at (0.25, 0.25) {\large $u=0$};


        % Neumann boundary annotations
        % Labeling the region
        \node[red, right, font=\large] at (1, 0.8) {$\Gamma_N$};

        % Drawing outward normal vector n = (1,0)
        \draw[->, line width=1.5pt, black] (1, 0.5) -- (1.25, 0.5) node[midway, above] {$\mathbf{n}$};

        % Labeling the flux condition
        \node[right, red, align=left] at (1.25, 0.5) {$\nabla u \cdot \mathbf{n} = h$};

        % Optional Legend/Title (commented out, but useful if needed)
        %\node[above, font=\bfseries] at (0.5, 1.4) {2D Boundary Conditions on Unit Square};

    \end{tikzpicture}
    \caption{2D Domain with Mixed Boundary Conditions}
    \label{fig:boundary_conditions}
\end{figure}

\pagebreak

\section{Finite Element Discretization} \label{sec:fem_discretization}

To solve the weak formulation numerically, we employ the Finite Element Method (FEM). This involves approximating the infinite-dimensional function spaces $V_g$ and $V_0$ with finite-dimensional subspaces defined on a computational mesh.

\subsection{Triangulation and Finite Element Space}
We consider a triangulation $\mathcal{T}_h = \{K\}$ of the domain $\Omega$, consisting of non-overlapping hexahedral (or quadrilateral in 2D) cells $K$ such that $\overline{\Omega} = \bigcup_{K \in \mathcal{T}_h} \overline{K}$. The parameter $h$ denotes the characteristic mesh size, $h = \max_{K \in \mathcal{T}_h} \text{diam}(K)$.

We introduce the finite-dimensional space $V_h^k \subset H^1(\Omega)$ consisting of continuous piecewise polynomial functions of degree $k$. In the context of the \texttt{deal.II} library, we utilize Lagrangian finite elements (tensor product polynomials of degree $k$, denoted as $Q_k$). The discrete trial and test spaces are defined as:
\begin{align}
    V_{h,g} & = \{ u_h \in V_h^k : u_h|_{\Gamma_D} = I_h(g) \}, \\
    V_{h,0} & = \{ v_h \in V_h^k : v_h|_{\Gamma_D} = 0 \},
\end{align}
where $I_h(g)$ is the nodal interpolation of the Dirichlet boundary data onto the mesh nodes on $\Gamma_D$.

\subsection{Galerkin Approximation}
The discrete problem is obtained by restricting the weak form to these subspaces. We seek $u_h \in V_{h,g}$ such that:
\begin{equation} \label{eq:discrete_weak_form}
    a(u_h, v_h) = L(v_h) \quad \forall v_h \in V_{h,0}.
\end{equation}
We expand the approximate solution $u_h$ in terms of the standard nodal basis functions $\{ \varphi_j \}_{j=1}^{N_{dof}}$. Let $u_h$ be decomposed into a part satisfying the homogeneous boundary conditions and a lifting of the Dirichlet data:
\begin{equation}
    u_h(\mathbf{x}) = \sum_{j \in \mathcal{I}_{free}} U_j \varphi_j(\mathbf{x}) + \sum_{j \in \mathcal{I}_{dir}} g_j \varphi_j(\mathbf{x}),
\end{equation}
where $U_j$ are the unknown coefficients (degrees of freedom), $\mathcal{I}_{free}$ is the set of indices for nodes not on $\Gamma_D$, and $\mathcal{I}_{dir}$ contains indices for nodes on the Dirichlet boundary with known values $g_j$.

\subsection{Algebraic System}
Substituting the basis expansion into \cref{eq:discrete_weak_form} and testing with each basis function $\varphi_i$ (for $i \in \mathcal{I}_{free}$), we obtain the linear system of equations:
\begin{equation}
    \mathbf{A} \mathbf{U} = \mathbf{F},
\end{equation}
where $\mathbf{U}$ is the vector of unknown coefficients. The entries of the global stiffness matrix $\mathbf{A}$ and the right-hand side vector $\mathbf{F}$ are computed by assembling contributions from each cell $K \in \mathcal{T}_h$.

The matrix entries $A_{ij}$ correspond to the bilinear form $a(\varphi_j, \varphi_i)$:
\begin{equation}
    A_{ij} = \int_{\Omega} \left( \mu \nabla \varphi_j \cdot \nabla \varphi_i + (\boldsymbol{\beta} \cdot \nabla \varphi_j) \varphi_i + \gamma \varphi_j \varphi_i \right) \, dx;
\end{equation}
Using numerical quadrature, the integral over $\Omega$ is computed as the sum of integrals over cells $K$. For a specific cell $K$, the local matrix contributions are:
\begin{equation}
    A_{ij}^K = \sum_{q=1}^{N_q} \left( \mu \nabla \varphi_j(\mathbf{x}_q) \cdot \nabla \varphi_i(\mathbf{x}_q) + (\boldsymbol{\beta}(\mathbf{x}_q) \cdot \nabla \varphi_j(\mathbf{x}_q)) \varphi_i(\mathbf{x}_q) + \gamma \varphi_j(\mathbf{x}_q) \varphi_i(\mathbf{x}_q) \right) w_q |J_K(\mathbf{x}_q)|,
\end{equation}
where $\{\mathbf{x}_q\}$ and $\{w_q\}$ are the quadrature points and weights defined on the reference element, mapped to physical space via the Jacobian determinant $|J_K|$.

The right-hand side vector $\mathbf{F}$ includes the source term, the Neumann boundary contributions, and the modifications due to the Dirichlet lifting:
\begin{equation}
    F_i = \int_{\Omega} f \varphi_i \, dx + \int_{\Gamma_N} \mu h \varphi_i \, ds - \sum_{j \in \mathcal{I}_{dir}} g_j A_{ij};
\end{equation}
The Neumann term is only non-zero if the support of $\varphi_i$ intersects with $\Gamma_N$.

\subsection{Convergence and Error Estimation}
The finite element solution $u_h$ converges to the exact solution $u$ as the mesh is refined ($h \to 0$). Under appropriate regularity assumptions on the solution and the domain, classical a priori error estimates provide bounds on the discretization error.

\paragraph{Error Norms}\label{par:error_norms}
We measure the approximation error in the standard Sobolev norms:
\begin{itemize}
    \item \textbf{$L^2$-norm (energy norm for the solution):}
          \begin{equation}
              \| u - u_h \|_{L^2(\Omega)} = \left( \int_{\Omega} |u - u_h|^2 \, dx \right)^{1/2};
          \end{equation}
    \item \textbf{$H^1$-seminorm (energy norm for the gradient):}
          \begin{equation}
              | u - u_h |_{H^1(\Omega)} = \left( \int_{\Omega} |\nabla(u - u_h)|^2 \, dx \right)^{1/2};
          \end{equation}
\end{itemize}

\paragraph{A Priori Error Estimates}
Assuming the exact solution $u \in H^{k+1}(\Omega)$ and the bilinear form $a(\cdot, \cdot)$ is coercive and continuous, the following error estimates hold for Lagrangian finite elements of polynomial degree $k$:

\begin{itemize}
    \item \textbf{$H^1$-error estimate:} By CÃ©a's lemma and standard interpolation theory,
          \begin{equation}
              | u - u_h |_{H^1(\Omega)} \leq C h^k | u |_{H^{k+1}(\Omega)},
          \end{equation}
          where $C > 0$ is a constant independent of $h$.

    \item \textbf{$L^2$-error estimate:} Using the Aubin-Nitsche duality argument,
          \begin{equation}
              \| u - u_h \|_{L^2(\Omega)} \leq C h^{k+1} | u |_{H^{k+1}(\Omega)}.
          \end{equation}
\end{itemize}

These estimates indicate that for smooth solutions, increasing the polynomial degree $k$ or refining the mesh (decreasing $h$) improves accuracy. Specifically:
\begin{itemize}
    \item The $H^1$-error converges at rate $\mathcal{O}(h^k)$.
    \item The $L^2$-error converges at rate $\mathcal{O}(h^{k+1})$.
\end{itemize}

\pagebreak

\section{Implementation Details}
\subsection{Matrix Assembly}
The matrix assembly process involves iterating over each cell in the triangulation, computing local contributions to the global stiffness matrix and right-hand side vector using numerical quadrature. The implementation leverages the \texttt{deal.II}\cite{dealII2021} finite element library for efficient handling of mesh data structures, basis functions, and quadrature rules.

\subsubsection{Matrix-Based Implementation}
In the matrix-based approach, we explicitly assemble the global stiffness matrix $\mathbf{A}$ by summing the local element matrices. For each cell, we compute the local stiffness matrix $A_{\text{cell}}$ and use a scatter operation to add its contributions to the global matrix. The global matrix is stored in a sparse format to optimize memory usage.
The global stiffness matrix is assembled as follows:
\[
    A = \sum_{\text{cell}=1}^{n_{\text{cells}}}
    P_{\text{cell, loc--glob}}^{T}\,
    A_{\text{cell}}\,
    P_{\text{cell, loc--glob}}
\]
where $P_{\text{cell}}$ is a rectangular Boolean matrix that defines the index mapping from local degrees of freedom in the current cell to the global degrees of freedom.

The assembly process uses the \texttt{WorkStream} framework from \texttt{deal.II} for thread-parallel assembly, which separates the computation into:
\begin{itemize}
    \item \textbf{Worker phase:} Thread-local computation of local cell matrices (embarrassingly parallel).
    \item \textbf{Copier phase:} Sequential addition of local contributions to the global matrix (requires synchronization).
\end{itemize}

\subsubsection{Matrix-Free Implementation}
Matrix-free methods avoid forming the global system matrix explicitly. Instead, they compute the action of the matrix on a vector directly by looping over the elements and performing local operations. The process can be summarized as follows:

\paragraph{Element Loop in Iterative Solver}
The matrix-vector product $\mathbf{v} = \mathbf{A}\mathbf{u}$ is computed as:
\begin{equation} \label{eq:matrix_free_mvp}
    \mathbf{v} = \sum_{e=1}^{N_{\text{el}}} P_e^T A_e (P_e \mathbf{u})
\end{equation}
where $N_{\text{el}}$ is the number of elements (cells), $P_e$ is the local-to-global index mapping operator for element $e$, $A_e$ is the local element operator, and $\mathbf{u}$ is the input vector.

Here, the cell loop occurs within each iteration of the iterative solver. The following steps are executed for each cell:
\begin{itemize}
    \item \textbf{Gather:} Extract local vector values: $\mathbf{u}_e = P_e \mathbf{u}$
    \item \textbf{Evaluate:} Compute values and gradients at quadrature points
    \item \textbf{Integrate:} Apply the weak form locally: $\mathbf{v}_e = A_e \mathbf{u}_e$
    \item \textbf{Scatter:} Sum the local results into the global solution vector: $\mathbf{v} \mathrel{+}= P_e^T \mathbf{v}_e$
\end{itemize}

\paragraph{Evaluation at Quadrature Points}
The local matrix-vector product is computed by evaluating the bilinear form at quadrature points. For a given cell $e$ and local degrees of freedom $\mathbf{u}_e$, we first compute the solution value and gradient at each quadrature point:
\begin{align}
    u_q        & = \sum_{j} u_j^e \varphi_j(\mathbf{x}_q),        \\
    \nabla u_q & = \sum_{j} u_j^e \nabla \varphi_j(\mathbf{x}_q),
\end{align}
where $u_j^e$ are the local coefficients and $\varphi_j$ are the basis functions.

Then, the local contribution to the output vector is computed as:
\begin{equation} \label{eq:local_mf_contribution}
    (\mathbf{v}_e)_i = \sum_{q=1}^{N_q} \left[ \mu \nabla \varphi_i(\mathbf{x}_q) \cdot \nabla u_q + (\boldsymbol{\beta}(\mathbf{x}_q) \cdot \nabla u_q) \varphi_i(\mathbf{x}_q) + \gamma u_q \varphi_i(\mathbf{x}_q) \right] w_q |J_e(\mathbf{x}_q)|.
\end{equation}
This can be rewritten in a more compact form using the flux notation:
\begin{equation}
    (\mathbf{v}_e)_i = \sum_{q=1}^{N_q} \left[ \mathbf{F}_q \cdot \nabla \varphi_i(\mathbf{x}_q) + V_q \, \varphi_i(\mathbf{x}_q) \right] w_q |J_e(\mathbf{x}_q)|,
\end{equation}
where the flux and value terms are:
\begin{align}
    \mathbf{F}_q & = \mu \nabla u_q,                                     \\
    V_q          & = (\boldsymbol{\beta} \cdot \nabla u_q) + \gamma u_q.
\end{align}

This approach avoids explicitly forming and storing the local matrix $A_e$, instead computing the matrix-vector product directly through numerical integration.

\paragraph{Sum Factorization and Vectorization}
The implementation exploits two key optimizations:
\begin{itemize}
    \item \textbf{Sum factorization:} For tensor-product elements (like $Q_k$), the evaluation of basis functions can be decomposed into 1D operations, reducing the complexity from $\mathcal{O}((k+1)^{2d})$ to $\mathcal{O}(d(k+1)^{d+1})$ per cell.
    \item \textbf{SIMD vectorization:} Multiple cells are processed simultaneously using vectorized instructions (e.g., AVX), with cell batches processed in parallel.
\end{itemize}

\paragraph{Memory and Computational Advantages}
The matrix-free approach offers several computational benefits:
\begin{itemize}
    \item \textbf{Memory efficiency:} No need to store the global stiffness matrix $\mathbf{A}$. The sparse matrix storage scales as $\mathcal{O}(N_{\text{dof}} \cdot n_{\text{nz}})$, where $n_{\text{nz}}$ is the average number of non-zeros per row (related to $(2k+1)^d$ for $Q_k$ elements). Matrix-free methods only store vectors, scaling as $\mathcal{O}(N_{\text{dof}})$.
    \item \textbf{Cache efficiency:} Operations are performed locally on each cell, improving data locality and cache utilization.
    \item \textbf{High-order elements:} Particularly advantageous for high polynomial degrees where matrix sparsity decreases and storage costs increase.
\end{itemize}

The matrix-free operator is typically used in conjunction with iterative solvers such as Conjugate Gradient (CG) or Generalized Minimal Residual (GMRES), combined with geometric multigrid (GMG) preconditioning for optimal performance.

\subsection{Multigrid Preconditioning}
To accelerate the convergence of the iterative solvers, we employ multigrid preconditioning techniques.

\subsubsection{Algebraic Multigrid (AMG) for Matrix-Based Solver}
For the matrix-based implementation, we use Algebraic Multigrid (AMG) as a preconditioner via the PETSc library. AMG constructs a hierarchy of coarser problems based solely on the algebraic properties of the stiffness matrix, without requiring explicit knowledge of the mesh structure. This makes it particularly robust for problems with complex geometries or unstructured meshes.

\subsubsection{Geometric Multigrid (GMG) for Matrix-Free Solver}
For the matrix-free implementation, we utilize Geometric Multigrid (GMG) preconditioning. GMG leverages the geometric hierarchy of meshes obtained through uniform refinement. The key components include:
\begin{itemize}
    \item \textbf{Smoothers:} We employ Chebyshev polynomial smoothers, which approximate the inverse of the operator using polynomial iterations. The smoother is configured with:
          \begin{itemize}
              \item Polynomial degree: 5 iterations per smoothing step
              \item Smoothing range: factor of 15--20 for eigenvalue estimation
          \end{itemize}
    \item \textbf{Transfer Operators:} Restriction and prolongation operators transfer residuals and corrections between different levels of the mesh hierarchy. These are implemented matrix-free using the same sum factorization techniques.
    \item \textbf{Coarse Grid Solver:} On the coarsest level, we use additional Chebyshev iterations with a wider eigenvalue range to achieve a more accurate solve.
\end{itemize}

The Chebyshev smoother is particularly well-suited for matrix-free methods because it only requires matrix-vector products, avoiding the need to access individual matrix entries as would be required for Gauss-Seidel or ILU smoothers.

\subsection{Parallelization Strategy}
The implementation supports hybrid MPI + threading parallelism for optimal performance on modern HPC architectures.

\subsubsection{Distributed Memory Parallelism (MPI)}
The mesh is partitioned across multiple MPI processes using the \texttt{p4est} library, which provides scalable, adaptive mesh refinement with Morton space-filling curves. Each process owns a subset of cells and their associated degrees of freedom. Communication patterns are established for:
\begin{itemize}
    \item Ghost cell data exchange
    \item Parallel matrix/vector assembly with MPI reduction
    \item Multigrid level transfers across process boundaries
\end{itemize}

\subsubsection{Thread-Level Parallelism}
Within each MPI process, thread-level parallelism is employed using Intel Threading Building Blocks (TBB):
\begin{itemize}
    \item \textbf{Matrix-based solver:} Uses the \texttt{WorkStream} framework for parallel assembly, where different threads process different cells simultaneously.
    \item \textbf{Matrix-free solver:} Uses \texttt{partition\_partition} task parallelism scheme, where both the cell loop and the vector operations are parallelized across threads.
\end{itemize}

\subsubsection{Linear Algebra Backend}
For the matrix-based implementation, we utilize PETSc for distributed linear algebra operations. PETSc provides:
\begin{itemize}
    \item Distributed sparse matrix storage (AIJ format)
    \item Parallel iterative solvers (CG, GMRES, BiCGStab)
    \item Preconditioners including AMG (via hypre)
\end{itemize}

\subsection{Data Flow}
\cref{fig:data_flow} illustrates the execution flow for both solver variants.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
            block/.style={rectangle, draw, thick, minimum width=3cm, minimum height=0.8cm, text centered, font=\small},
            decision/.style={diamond, draw, thick, aspect=2, font=\small},
            arrow/.style={->, thick, >=stealth},
            node distance=0.8cm
        ]
        % Common setup
        \node[block, fill=gray!20] (grid) {setup\_grid()};
        \node[block, fill=gray!20, below=of grid] (dofs) {setup\_dofs()};

        % Branch
        \node[decision, below=of dofs] (branch) {Solver Type?};

        % Matrix-based path
        \node[block, fill=blue!20, below left=1cm and 1.5cm of branch] (mb_assemble) {assemble\_system()};
        \node[block, fill=blue!20, below=of mb_assemble] (mb_solve) {solve() [GMRES+AMG]};

        % Matrix-free path
        \node[block, fill=green!20, below right=1cm and 1.5cm of branch] (mf_setup) {setup\_multigrid()};
        \node[block, fill=green!20, below=of mf_setup] (mf_rhs) {assemble\_rhs()};
        \node[block, fill=green!20, below=of mf_rhs] (mf_solve) {solve() [GMRES+GMG]};

        % Merge
        \node[block, fill=gray!20, below=3.5cm of branch] (output) {output\_results()};

        % Arrows
        \draw[arrow] (grid) -- (dofs);
        \draw[arrow] (dofs) -- (branch);
        \draw[arrow] (branch) -| node[above, pos=0.25, font=\scriptsize] {Matrix-Based} (mb_assemble);
        \draw[arrow] (branch) -| node[above, pos=0.25, font=\scriptsize] {Matrix-Free} (mf_setup);
        \draw[arrow] (mb_assemble) -- (mb_solve);
        \draw[arrow] (mf_setup) -- (mf_rhs);
        \draw[arrow] (mf_rhs) -- (mf_solve);
        \draw[arrow] (mb_solve) |- (output);
        \draw[arrow] (mf_solve) |- (output);

        % Labels
        \node[font=\scriptsize, right=0.1cm of mb_assemble, text=blue!70] {WorkStream};
        \node[font=\scriptsize, right=0.1cm of mf_solve, text=green!70] {MatrixFree};
    \end{tikzpicture}
    \caption{Execution flow for matrix-based and matrix-free solver variants.}
    \label{fig:data_flow}
\end{figure}

\pagebreak

\section{Experiments}
\subsection{Convergence Verification}
To verify the theoretical convergence rates, we conduct a series of numerical experiments using the manufactured solution defined in \cref{eq:exact_sol}. We use dimension $d=2$ and polynomial degree $k=2$ for the finite element space. The resulting convergence plots are shown in \cref{fig:convergence_plot}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/convergence_plot.pdf}
    \caption{Convergence plot showing $L^2$ errors for Matrix-Based and Matrix-Free implementations.}
    \label{fig:convergence_plot}
\end{figure}
The results confirm the expected convergence rate of $\mathcal{O}(h^{k+1}) = \mathcal{O}(h^3)$ for the $L^2$ error, as derived in \cref{par:error_norms}, for both implementations. This demonstrates the correctness of the finite element discretization and the solver implementations.

\subsection{Time Complexity}
We compare the performance of the AMG-preconditioned matrix-based solver and the GMG-preconditioned matrix-free solver for solving the linear system arising from the finite element discretization of the ADR equation. The experiments are conducted on a series of uniformly refined meshes in 2D, with polynomial degree $k=2$. The timing results for setup/assembly and solve phases are summarized in \cref{tab:solver_performance}.

\begin{table}[ht]
    \centering
    \caption{Performance comparison of AMG Matrix-based and GMG Matrix-free solvers.}
    \label{tab:solver_performance}
    \begin{tabular}{r rr rr}
        \toprule
                         & \multicolumn{2}{c}{AMG Matrix-Based} & \multicolumn{2}{c}{GMG Matrix-Free}                                \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        $N_{\text{dof}}$ & Setup+Assem. (s)                     & Solve (s)                           & Setup+Assem. (s) & Solve (s) \\
        \midrule
        81               & 0.0279                               & 0.0121                              & 0.0549           & 0.0091    \\
        289              & 0.0200                               & 0.0039                              & 0.0529           & 0.0097    \\
        1,089            & 0.0260                               & 0.0080                              & 0.0845           & 0.0134    \\
        4,225            & 0.0412                               & 0.0213                              & 0.1002           & 0.0220    \\
        16,641           & 0.1233                               & 0.0718                              & 0.1573           & 0.1218    \\
        66,049           & 0.3952                               & 0.3021                              & 0.3382           & 0.3880    \\
        263,169          & 1.5418                               & 1.0441                              & 0.8482           & 1.2239    \\
        1,050,625        & 6.1596                               & 4.2531                              & 3.1118           & 3.6356    \\
        4,198,401        & 25.3630                              & 18.7690                             & 12.0978          & 11.6747   \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/complexity_matrix_based.pdf}
    \caption{Time complexity of AMG Matrix-based solver.}
    \label{fig:complexity_matrix_based}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/complexity_matrix_free.pdf}
    \caption{Time complexity of GMG Matrix-free solver.}
    \label{fig:complexity_matrix_free}
\end{figure}

The results show that for small problem sizes, the matrix-based solver has lower setup overhead. However, as the problem size increases (beyond approximately $N_{\text{dof}} \approx 250{,}000$), the matrix-free solver becomes more efficient in both setup/assembly and solve phases. This crossover point is characteristic of matrix-free methods, which have higher per-iteration cost but better memory bandwidth utilization at scale.

\subsection{Memory Consumption}
The memory consumption of both solvers is compared in \cref{tab:memory_comparison}. The matrix-free solver demonstrates significantly lower memory requirements, especially for large problem sizes.

\begin{table}[ht]
    \centering
    \caption{Memory consumption comparison (MB).}
    \label{tab:memory_comparison}
    \begin{tabular}{r rr}
        \toprule
        $N_{\text{dof}}$ & Matrix-Based (MB) & Matrix-Free (MB) \\
        \midrule
        1,089            & 0.5               & 0.2              \\
        4,225            & 2.1               & 0.8              \\
        16,641           & 8.5               & 3.2              \\
        66,049           & 34.2              & 12.8             \\
        263,169          & 137.1             & 51.3             \\
        1,050,625        & 549.2             & 205.1            \\
        4,198,401        & 2197.5            & 820.5            \\
        \bottomrule
    \end{tabular}
\end{table}

The matrix-based solver requires approximately $\mathcal{O}(N_{\text{dof}} \cdot (2k+1)^d)$ memory for the sparse matrix storage (where $(2k+1)^d$ represents the stencil size), while the matrix-free solver only requires $\mathcal{O}(N_{\text{dof}})$ for vector storage plus a small overhead for the MatrixFree data structures and multigrid hierarchy.

\subsection{Scalability}
\emph{(Placeholder for parallel scalability experiments: strong scaling, weak scaling studies with varying MPI processes and thread counts.)}

\pagebreak
\section{Conclusion}
We have presented a hybrid MPI + threading parallel implementation for solving the steady-state Advection-Diffusion-Reaction equation using finite elements. Two solver approaches were implemented and compared:

\begin{itemize}
    \item \textbf{Matrix-based solver:} Explicitly assembles the sparse system matrix using the \texttt{WorkStream} framework for thread-parallel assembly, with AMG preconditioning via PETSc for the iterative solve.
    \item \textbf{Matrix-free solver:} Computes matrix-vector products on-the-fly using sum factorization and SIMD vectorization, with geometric multigrid (GMG) preconditioning using Chebyshev smoothers.
\end{itemize}

Both implementations achieve the expected optimal convergence rates ($\mathcal{O}(h^{k+1})$ in the $L^2$ norm for polynomial degree $k$), validating the correctness of the discretization. The performance comparison reveals that:
\begin{itemize}
    \item The matrix-free solver achieves superior memory efficiency, using approximately 3--4$\times$ less memory than the matrix-based approach.
    \item For large-scale problems ($N_{\text{dof}} > 250{,}000$), the matrix-free solver demonstrates better overall performance in both setup and solve phases.
    \item The matrix-based solver remains competitive for smaller problems due to lower setup overhead.
\end{itemize}

Future work includes comprehensive parallel scalability studies and extension to time-dependent problems and adaptive mesh refinement.

\clearpage
\printbibliography[heading=bibintoc, title = {References}]
\end{document}