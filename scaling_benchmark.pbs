#!/bin/bash

#===============================================================================
# PBS Job Script for Scaling Benchmark
#===============================================================================
# Key features:
# 1. Uses larger problem sizes (refs 9-11 for 2D) to ensure meaningful scaling
# 2. Simplified configurations - pure MPI scaling as primary test
# 3. Generates publication-quality plots matching reference style
#===============================================================================

#PBS -N adr_scaling_benchmark
#PBS -l select=1:ncpus=28
#PBS -l walltime=24:00:00
#PBS -q cpu
#PBS -A Hybrid-Parallelism-ADR-Solver-PDE2025
#PBS -m abe
#PBS -M peng.rao@mail.polimi.it

#===============================================================================
# Setup
#===============================================================================

cd ${PBS_O_WORKDIR:-$(pwd)}

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
LOG_DIR="logs_scaling"
mkdir -p ${LOG_DIR}
LOG_FILE="${LOG_DIR}/scaling_${PBS_JOBID:-local}_${TIMESTAMP}.log"

RESULTS_DIR="results_scaling_${TIMESTAMP}"
mkdir -p ${RESULTS_DIR}

exec > >(tee -a "${LOG_FILE}") 2>&1

echo "==============================================================================="
echo "SCALING BENCHMARK"
echo "==============================================================================="
echo "Job ID: ${PBS_JOBID}"
echo "Date: $(date)"
echo "Results Dir: ${RESULTS_DIR}"
echo "==============================================================================="

#-------------------------------------------------------------------------------
# Spack Environment
#-------------------------------------------------------------------------------
if [ -f "/work/u11022931/spack/share/spack/setup-env.sh" ]; then
    source /work/u11022931/spack/share/spack/setup-env.sh
elif [ -f "/opt/spack/share/spack/setup-env.sh" ]; then
    source /opt/spack/share/spack/setup-env.sh
else
    echo "ERROR: Spack not found"
    exit 1
fi

spack env activate .
echo "Spack Environment: $(spack env status)"
echo ""

ulimit -s unlimited

#===============================================================================
# Build
#===============================================================================

BUILD_DIR="build"
EXECUTABLE="${BUILD_DIR}/scaling_study"
REBUILD_REQUIRED=false

if [ ! -f "$EXECUTABLE" ]; then
    REBUILD_REQUIRED=true
else
    NEWEST_SOURCE=$(find . -maxdepth 2 \( -name "*.cpp" -o -name "*.h" -o -name "CMakeLists.txt" \) 2>/dev/null | xargs ls -t 2>/dev/null | head -n 1)
    if [ -n "$NEWEST_SOURCE" ] && [ "$NEWEST_SOURCE" -nt "$EXECUTABLE" ]; then
        REBUILD_REQUIRED=true
    fi
fi

if [ "$REBUILD_REQUIRED" = true ]; then
    echo "Building scaling benchmark..."
    mkdir -p ${BUILD_DIR}
    cd ${BUILD_DIR}
    cmake .. -DCMAKE_BUILD_TYPE=Release
    make -j 28 scaling_study || exit 1
    cd ..
    echo "Build complete."
else
    echo "Binary is up to date."
fi

#===============================================================================
# Benchmark Parameters
#===============================================================================

DIM=2
DEGREE=2
WARMUP=1
TRIALS=3

# Problem sizes for 2D Q2 elements:
#   refs=9  -> ~1.05M DoFs
#   refs=10 -> ~4.2M DoFs
#   refs=11 -> ~16.8M DoFs

MIN_REF_STRONG=9
MAX_REF_STRONG=11

MIN_REF_WEAK=7
MAX_REF_WEAK=9

MPI_CMD="mpirun"

echo ""
echo "==============================================================================="
echo "Problem Size Estimates (2D Q2 elements)"
echo "==============================================================================="
echo "refs=9:  ~1,050,000 DoFs"
echo "refs=10: ~4,200,000 DoFs"
echo "refs=11: ~16,800,000 DoFs"
echo ""
echo "Selected range: refs ${MIN_REF_STRONG}-${MAX_REF_STRONG}"
echo "==============================================================================="

#===============================================================================
# PHASE 1: PURE MPI STRONG SCALING
#===============================================================================

echo ""
echo "==============================================================================="
echo "PHASE 1: PURE MPI STRONG SCALING"
echo "==============================================================================="

# Pure MPI configurations (1 thread each)
declare -a PURE_MPI_CONFIGS=(
    "1 1"
    "2 1"
    "4 1"
    "7 1"
    "14 1"
    "28 1"
)

for CONFIG in "${PURE_MPI_CONFIGS[@]}"; do
    read -r NRANKS NTHREADS <<< "$CONFIG"
    TOTAL_CORES=$((NRANKS * NTHREADS))
    CONFIG_NAME="pure_mpi_${NRANKS}"

    echo ""
    echo "-----------------------------------------------------------------------"
    echo "Pure MPI: ${NRANKS} processes × ${NTHREADS} thread = ${TOTAL_CORES} cores"
    echo "-----------------------------------------------------------------------"

    export TBB_NUM_THREADS=${NTHREADS}
    export OMP_NUM_THREADS=${NTHREADS}
    export OMP_PROC_BIND=close
    export OMP_PLACES=cores

    OUTPUT_PREFIX="${RESULTS_DIR}/strong_${CONFIG_NAME}"

    ${MPI_CMD} -n ${NRANKS} \
        ${EXECUTABLE} \
        --strong \
        --min-ref ${MIN_REF_STRONG} \
        --max-ref ${MAX_REF_STRONG} \
        --degree ${DEGREE} \
        --dim ${DIM} \
        --output "${OUTPUT_PREFIX}" \
        --threads ${NTHREADS} \
        --warmup ${WARMUP} \
        --trials ${TRIALS} \
        2>&1 | tee "${OUTPUT_PREFIX}.log"

    sleep 2
done

#===============================================================================
# PHASE 2: PURE THREADING SCALING
#===============================================================================

echo ""
echo "==============================================================================="
echo "PHASE 2: PURE THREADING SCALING"
echo "==============================================================================="

declare -a PURE_THREAD_CONFIGS=(
    "1 2"
    "1 4"
    "1 7"
    "1 14"
    "1 28"
)

for CONFIG in "${PURE_THREAD_CONFIGS[@]}"; do
    read -r NRANKS NTHREADS <<< "$CONFIG"
    TOTAL_CORES=$((NRANKS * NTHREADS))
    CONFIG_NAME="pure_thread_${NTHREADS}"

    echo ""
    echo "-----------------------------------------------------------------------"
    echo "Pure Threading: ${NRANKS} process × ${NTHREADS} threads = ${TOTAL_CORES} cores"
    echo "-----------------------------------------------------------------------"

    export TBB_NUM_THREADS=${NTHREADS}
    export OMP_NUM_THREADS=${NTHREADS}
    export OMP_PROC_BIND=spread
    export OMP_PLACES=cores

    OUTPUT_PREFIX="${RESULTS_DIR}/strong_${CONFIG_NAME}"

    ${MPI_CMD} -n ${NRANKS} \
        ${EXECUTABLE} \
        --strong \
        --min-ref ${MIN_REF_STRONG} \
        --max-ref ${MAX_REF_STRONG} \
        --degree ${DEGREE} \
        --dim ${DIM} \
        --output "${OUTPUT_PREFIX}" \
        --threads ${NTHREADS} \
        --warmup ${WARMUP} \
        --trials ${TRIALS} \
        2>&1 | tee "${OUTPUT_PREFIX}.log"

    sleep 2
done

#===============================================================================
# PHASE 3: WEAK SCALING
#===============================================================================

echo ""
echo "==============================================================================="
echo "PHASE 3: WEAK SCALING (Pure MPI)"
echo "==============================================================================="

for CONFIG in "${PURE_MPI_CONFIGS[@]}"; do
    read -r NRANKS NTHREADS <<< "$CONFIG"
    CONFIG_NAME="weak_mpi_${NRANKS}"

    echo ""
    echo "-----------------------------------------------------------------------"
    echo "Weak Scaling: ${NRANKS} processes"
    echo "-----------------------------------------------------------------------"

    export TBB_NUM_THREADS=${NTHREADS}
    export OMP_NUM_THREADS=${NTHREADS}

    OUTPUT_PREFIX="${RESULTS_DIR}/${CONFIG_NAME}"

    ${MPI_CMD} -n ${NRANKS} \
        ${EXECUTABLE} \
        --weak \
        --min-ref ${MIN_REF_WEAK} \
        --max-ref ${MAX_REF_WEAK} \
        --degree ${DEGREE} \
        --dim ${DIM} \
        --output "${OUTPUT_PREFIX}" \
        --threads ${NTHREADS} \
        --warmup ${WARMUP} \
        --trials ${TRIALS} \
        2>&1 | tee "${OUTPUT_PREFIX}.log"

    sleep 2
done

#===============================================================================
# Combine Results
#===============================================================================

echo ""
echo "==============================================================================="
echo "Combining Results"
echo "==============================================================================="

COMBINED_ALL="${RESULTS_DIR}/combined_all_results.csv"

first_file=true
for f in ${RESULTS_DIR}/*.csv; do
    if [ -f "$f" ] && [ "$f" != "${COMBINED_ALL}" ]; then
        if [ "$first_file" = true ]; then
            cat "$f" > ${COMBINED_ALL}
            first_file=false
        else
            tail -n +2 "$f" >> ${COMBINED_ALL}
        fi
    fi
done

echo "Created: ${COMBINED_ALL}"

#===============================================================================
# Generate Analysis Script
#===============================================================================

ANALYSIS_SCRIPT="${RESULTS_DIR}/analyze_scaling.py"

cat > ${ANALYSIS_SCRIPT} << 'PYTHON_EOF'
#!/usr/bin/env python3
"""
Scaling Analysis Script - Generates strong scaling plots for total time
Matches the style of the reference plot with multiple solvers and problem sizes
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import sys
import os

# Set plot style
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.dpi'] = 150
plt.rcParams['font.size'] = 11
plt.rcParams['axes.facecolor'] = '#e8f5e9'  # Light green background
plt.rcParams['figure.facecolor'] = '#e8f5e9'

def load_data(filename):
    """Load CSV data."""
    return pd.read_csv(filename)

def format_dofs(n_dofs):
    """Format DoF count for legend (e.g., 1.1M, 67.1M)."""
    if n_dofs >= 1e6:
        return f"{n_dofs/1e6:.1f}M"
    elif n_dofs >= 1e3:
        return f"{n_dofs/1e3:.0f}K"
    else:
        return str(n_dofs)

def plot_strong_scaling_total_time(df, output_dir):
    """
    Plot strong scaling for total time for all solvers.
    Matches the reference plot style.
    """
    fig, ax = plt.subplots(figsize=(10, 7))
    
    # Filter strong scaling data
    strong_df = df[df['test_type'] == 'strong_scaling'].copy()
    
    if strong_df.empty:
        print("No strong scaling data found")
        return
    
    # Define colors for different solver/size combinations
    color_palette = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 
                     'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray']
    
    plot_data = []
    
    # Collect all solver/refinement combinations
    for solver in strong_df['solver_type'].unique():
        solver_df = strong_df[strong_df['solver_type'] == solver]
        
        for refs in sorted(solver_df['n_refinements'].unique(), reverse=True):
            ref_df = solver_df[solver_df['n_refinements'] == refs]
            
            # Group by total cores
            grouped = ref_df.groupby('total_cores').agg({
                'total_time_avg': 'mean',
                'solve_time_avg': 'mean',
                'n_dofs': 'first'
            }).reset_index()
            
            grouped = grouped.sort_values('total_cores')
            
            if len(grouped) >= 2:
                n_dofs = grouped['n_dofs'].iloc[0]
                plot_data.append({
                    'solver': solver,
                    'refs': refs,
                    'n_dofs': n_dofs,
                    'cores': grouped['total_cores'].values,
                    'total_time': grouped['total_time_avg'].values,
                    'solve_time': grouped['solve_time_avg'].values,
                })
    
    # Sort by problem size (largest first)
    plot_data.sort(key=lambda x: x['n_dofs'], reverse=True)
    
    # Plot each line
    handles = []
    labels = []
    
    for idx, data in enumerate(plot_data):
        solver_abbrev = 'mf' if data['solver'] == 'matrix_free' else 'mb'
        dofs_str = format_dofs(data['n_dofs'])
        label = f"{solver_abbrev} ({dofs_str} DoFs)"
        
        color = color_palette[idx % len(color_palette)]
        marker = 'o'
        
        line, = ax.plot(data['cores'], data['total_time'],
                       marker=marker, linestyle='-', color=color,
                       markersize=8, linewidth=2, label=label)
        
        handles.append(line)
        labels.append(label)
    
    # Add ideal scaling line
    if plot_data:
        ref_data = plot_data[0]
        ref_time = ref_data['total_time'][0]
        ref_cores = ref_data['cores'][0]
        
        max_cores = max(d['cores'].max() for d in plot_data)
        min_cores = min(d['cores'].min() for d in plot_data)
        
        ideal_cores = np.linspace(min_cores, max_cores, 50)
        ideal_times = ref_time * (ref_cores / ideal_cores)
        
        ideal_line, = ax.plot(ideal_cores, ideal_times, 'k--', linewidth=2, label='Ideal scaling')
        handles.append(ideal_line)
        labels.append('Ideal scaling')
    
    # Configure axes
    ax.set_yscale('log')
    
    ax.set_xlabel('Number of processors', fontsize=12)
    ax.set_ylabel('Time (seconds)', fontsize=12)
    ax.set_title('Strong scaling for total time for all the solvers', fontsize=14, fontweight='bold')
    
    # Set x-axis ticks
    all_cores = sorted(set().union(*[set(d['cores']) for d in plot_data]))
    ax.set_xticks(all_cores)
    ax.set_xticklabels([str(int(c)) for c in all_cores])
    
    # Legend
    ax.legend(handles=handles, labels=labels, loc='upper right', 
              fontsize=10, framealpha=0.9, facecolor='white')
    
    # Grid
    ax.grid(True, which='both', linestyle='-', alpha=0.5)
    ax.set_axisbelow(True)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'strong_scaling_total_time.png'), 
                dpi=150, bbox_inches='tight', facecolor='#e8f5e9')
    plt.close()
    print("Saved: strong_scaling_total_time.png")


def plot_speedup_and_efficiency(df, output_dir):
    """Plot speedup and parallel efficiency."""
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    strong_df = df[df['test_type'] == 'strong_scaling'].copy()
    
    if strong_df.empty:
        return
    
    color_palette = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 
                     'tab:purple', 'tab:brown']
    
    plot_idx = 0
    
    for solver in strong_df['solver_type'].unique():
        solver_df = strong_df[strong_df['solver_type'] == solver]
        
        for refs in sorted(solver_df['n_refinements'].unique(), reverse=True)[:2]:
            ref_df = solver_df[solver_df['n_refinements'] == refs]
            
            grouped = ref_df.groupby('total_cores').agg({
                'total_time_avg': 'mean',
                'n_dofs': 'first'
            }).reset_index().sort_values('total_cores')
            
            if len(grouped) < 2:
                continue
            
            cores = grouped['total_cores'].values
            times = grouped['total_time_avg'].values
            n_dofs = grouped['n_dofs'].iloc[0]
            
            # Compute speedup and efficiency
            t1 = times[0]
            c1 = cores[0]
            speedup = t1 / times
            efficiency = speedup / (cores / c1)
            
            solver_abbrev = 'mf' if solver == 'matrix_free' else 'mb'
            dofs_str = format_dofs(n_dofs)
            label = f"{solver_abbrev} ({dofs_str} DoFs)"
            color = color_palette[plot_idx % len(color_palette)]
            
            axes[0].plot(cores, speedup, 'o-', color=color, label=label, 
                        markersize=8, linewidth=2)
            axes[1].plot(cores, efficiency * 100, 'o-', color=color, label=label,
                        markersize=8, linewidth=2)
            
            plot_idx += 1
    
    # Ideal lines
    max_cores = strong_df['total_cores'].max()
    min_cores = strong_df['total_cores'].min()
    
    axes[0].plot([min_cores, max_cores], [1, max_cores/min_cores], 'k--', 
                linewidth=2, label='Ideal')
    axes[1].axhline(y=100, color='k', linestyle='--', linewidth=2, label='Ideal (100%)')
    
    axes[0].set_xlabel('Number of processors', fontsize=12)
    axes[0].set_ylabel('Speedup', fontsize=12)
    axes[0].set_title('Strong Scaling: Speedup', fontsize=14, fontweight='bold')
    axes[0].legend(fontsize=9)
    axes[0].grid(True, alpha=0.5)
    
    axes[1].set_xlabel('Number of processors', fontsize=12)
    axes[1].set_ylabel('Parallel Efficiency (%)', fontsize=12)
    axes[1].set_title('Strong Scaling: Parallel Efficiency', fontsize=14, fontweight='bold')
    axes[1].legend(fontsize=9)
    axes[1].grid(True, alpha=0.5)
    axes[1].set_ylim([0, 120])
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'speedup_and_efficiency.png'),
                dpi=150, bbox_inches='tight', facecolor='#e8f5e9')
    plt.close()
    print("Saved: speedup_and_efficiency.png")


def plot_weak_scaling(df, output_dir):
    """Plot weak scaling results."""
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    weak_df = df[df['test_type'] == 'weak_scaling'].copy()
    if weak_df.empty:
        print("No weak scaling data")
        return
    
    colors = {'matrix_free': 'tab:blue', 'matrix_based': 'tab:orange'}
    
    for solver in weak_df['solver_type'].unique():
        solver_df = weak_df[weak_df['solver_type'] == solver]
        grouped = solver_df.groupby('total_cores').agg({
            'total_time_avg': 'mean'
        }).reset_index().sort_values('total_cores')
        
        if len(grouped) < 2:
            continue
        
        color = colors.get(solver, 'tab:gray')
        label = 'mf' if solver == 'matrix_free' else 'mb'
        
        axes[0].plot(grouped['total_cores'], grouped['total_time_avg'], 
                    'o-', label=label, color=color, markersize=8, linewidth=2)
        
        baseline = grouped['total_time_avg'].iloc[0]
        efficiency = baseline / grouped['total_time_avg']
        axes[1].plot(grouped['total_cores'], efficiency * 100,
                    'o-', label=label, color=color, markersize=8, linewidth=2)
    
    axes[0].set_xlabel('Number of processors', fontsize=12)
    axes[0].set_ylabel('Time (seconds)', fontsize=12)
    axes[0].set_title('Weak Scaling: Total Time', fontsize=14, fontweight='bold')
    axes[0].legend()
    axes[0].grid(True, alpha=0.5)
    
    axes[1].axhline(y=100, color='k', linestyle='--', linewidth=2, label='Ideal')
    axes[1].set_xlabel('Number of processors', fontsize=12)
    axes[1].set_ylabel('Efficiency (%)', fontsize=12)
    axes[1].set_title('Weak Scaling: Efficiency', fontsize=14, fontweight='bold')
    axes[1].legend()
    axes[1].grid(True, alpha=0.5)
    axes[1].set_ylim([0, 120])
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'weak_scaling.png'),
                dpi=150, bbox_inches='tight', facecolor='#e8f5e9')
    plt.close()
    print("Saved: weak_scaling.png")


def print_summary(df):
    """Print data summary."""
    print("\n" + "="*70)
    print("DATA SUMMARY")
    print("="*70)
    print(f"Total records: {len(df)}")
    print(f"Solvers: {df['solver_type'].unique().tolist()}")
    print(f"Test types: {df['test_type'].unique().tolist()}")
    
    if 'n_dofs' in df.columns:
        print(f"DoFs range: {df['n_dofs'].min():,} - {df['n_dofs'].max():,}")
    
    if 'total_cores' in df.columns:
        print(f"Core counts: {sorted(df['total_cores'].unique())}")
    
    print("\nStrong scaling summary:")
    strong_df = df[df['test_type'] == 'strong_scaling']
    for solver in strong_df['solver_type'].unique():
        solver_df = strong_df[strong_df['solver_type'] == solver]
        for refs in sorted(solver_df['n_refinements'].unique()):
            ref_df = solver_df[solver_df['n_refinements'] == refs].sort_values('total_cores')
            if len(ref_df) >= 2:
                t1 = ref_df['total_time_avg'].iloc[0]
                tn = ref_df['total_time_avg'].iloc[-1]
                c1 = ref_df['total_cores'].iloc[0]
                cn = ref_df['total_cores'].iloc[-1]
                n_dofs = ref_df['n_dofs'].iloc[0]
                speedup = t1 / tn
                ideal = cn / c1
                eff = speedup / ideal * 100
                print(f"  {solver} refs={refs} ({format_dofs(n_dofs)}): "
                      f"T({c1})={t1:.2f}s -> T({cn})={tn:.2f}s, "
                      f"Speedup={speedup:.1f}x, Eff={eff:.0f}%")


def main():
    if len(sys.argv) < 2:
        print("Usage: python analyze_scaling.py <results_csv>")
        sys.exit(1)
    
    csv_file = sys.argv[1]
    output_dir = os.path.dirname(csv_file) or '.'
    
    print(f"Loading: {csv_file}")
    df = load_data(csv_file)
    
    print_summary(df)
    
    print("\nGenerating plots...")
    plot_strong_scaling_total_time(df, output_dir)
    plot_speedup_and_efficiency(df, output_dir)
    plot_weak_scaling(df, output_dir)
    
    print("\nAll plots generated successfully!")


if __name__ == "__main__":
    main()
PYTHON_EOF

chmod +x ${ANALYSIS_SCRIPT}

#===============================================================================
# Run Analysis
#===============================================================================

echo ""
echo "==============================================================================="
echo "Running Analysis"
echo "==============================================================================="

python3 ${ANALYSIS_SCRIPT} ${COMBINED_ALL}

#===============================================================================
# Final Summary
#===============================================================================

echo ""
echo "==============================================================================="
echo "BENCHMARK COMPLETE"
echo "==============================================================================="
echo "Results: ${RESULTS_DIR}"
echo ""
echo "Key output files:"
echo "  ${COMBINED_ALL}"
echo "  ${RESULTS_DIR}/strong_scaling_total_time.png"
echo "  ${RESULTS_DIR}/speedup_and_efficiency.png"
echo "  ${RESULTS_DIR}/weak_scaling.png"
echo ""
echo "To regenerate plots:"
echo "  python ${ANALYSIS_SCRIPT} ${COMBINED_ALL}"
echo ""
echo "End: $(date)"
echo "==============================================================================="
